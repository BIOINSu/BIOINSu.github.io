<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/maze.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/maze.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/maze.png">
  <link rel="mask-icon" href="/images/maze.png" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="Flink中对无界数据进行计算（流处理）。">
<meta property="og:type" content="article">
<meta property="og:title" content="05 Flink DataStream">
<meta property="og:url" content="http://example.com/2021/05/09/Flink05_DataStreamAPI/index.html">
<meta property="og:site_name" content="BIOINSu">
<meta property="og:description" content="Flink中对无界数据进行计算（流处理）。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2021/05/09/Flink05_DataStreamAPI/Blog/source/_posts/Flink05_DataStreamAPI/0.png">
<meta property="og:image" content="http://example.com/2021/05/09/Flink05_DataStreamAPI/Blog/source/_posts/Flink05_DataStreamAPI/1.png">
<meta property="og:image" content="http://example.com/2021/05/09/Flink05_DataStreamAPI/Blog/source/_posts/Flink05_DataStreamAPI/2.png">
<meta property="og:image" content="http://example.com/2021/05/09/Flink05_DataStreamAPI/Blog/source_posts/Flink05_DataStreamAPI/3.png">
<meta property="article:published_time" content="2021-05-09T03:52:17.639Z">
<meta property="article:modified_time" content="2021-05-11T01:18:58.739Z">
<meta property="article:author" content="BIOINSu">
<meta property="article:tag" content="Flink">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/05/09/Flink05_DataStreamAPI/Blog/source/_posts/Flink05_DataStreamAPI/0.png">


<link rel="canonical" href="http://example.com/2021/05/09/Flink05_DataStreamAPI/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>05 Flink DataStream | BIOINSu</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">BIOINSu</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%81%E5%A4%84%E7%90%86%E7%9A%84wordcount"><span class="nav-number">1.</span> <span class="nav-text">流处理的WordCount</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#source"><span class="nav-number">2.</span> <span class="nav-text">Source</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#custom-source-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="nav-number">2.1.</span> <span class="nav-text">Custom Source 自定义数据源</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9D%9E%E5%B9%B6%E8%A1%8C%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="nav-number">2.1.1.</span> <span class="nav-text">非并行数据源</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="nav-number">2.1.2.</span> <span class="nav-text">并行数据源</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#mysql%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="nav-number">2.1.3.</span> <span class="nav-text">MySQL数据源</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#kafka%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="nav-number">2.1.4.</span> <span class="nav-text">Kafka数据源</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#socket-source"><span class="nav-number">2.2.</span> <span class="nav-text">Socket Source</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9B%86%E5%90%88source"><span class="nav-number">2.3.</span> <span class="nav-text">集合Source</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%87%E4%BB%B6source"><span class="nav-number">2.4.</span> <span class="nav-text">文件Source</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transformation"><span class="nav-number">3.</span> <span class="nav-text">Transformation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#keyby"><span class="nav-number">3.1.</span> <span class="nav-text">keyby</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#connect"><span class="nav-number">3.2.</span> <span class="nav-text">connect</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#splitselect"><span class="nav-number">3.3.</span> <span class="nav-text">split+select</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sink"><span class="nav-number">4.</span> <span class="nav-text">Sink</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#mysql-sink"><span class="nav-number">4.1.</span> <span class="nav-text">MySQL Sink</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#kafka-sink"><span class="nav-number">4.2.</span> <span class="nav-text">Kafka Sink</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#redis-sink"><span class="nav-number">4.3.</span> <span class="nav-text">Redis Sink</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">5.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">BIOINSu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">55</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">35</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/BIOINSu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;BIOINSu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/09/Flink05_DataStreamAPI/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="BIOINSu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BIOINSu">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          05 Flink DataStream
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-05-09 11:52:17" itemprop="dateCreated datePublished" datetime="2021-05-09T11:52:17+08:00">2021-05-09</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-05-11 09:18:58" itemprop="dateModified" datetime="2021-05-11T09:18:58+08:00">2021-05-11</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Flink/" itemprop="url" rel="index"><span itemprop="name">Flink</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>Flink中对无界数据进行计算（流处理）。</p>
<a id="more"></a>
<h3 id="流处理的wordcount">流处理的WordCount</h3>
<p>与批处理对比：</p>
<p>1、运行环境对象不同，StreamExecutionEnvironment。</p>
<p>2、某些算子不同，如批处理中按key分组是groupby，流处理中按key分组是keyby。</p>
<p>3、程序是一直运行，除非手动停止。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.<span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">KeyedStream</span>, <span class="type">StreamExecutionEnvironment</span>, <span class="type">WindowedStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">流式计算的wordcount</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     1 创建一个流处理的运行环境</span></span><br><span class="line"><span class="comment">     2 构建socket source数据源</span></span><br><span class="line"><span class="comment">     3 接收到的数据转为（单词，1）</span></span><br><span class="line"><span class="comment">     4 对元组使用keyby分组（类似于批处理中的groupby）</span></span><br><span class="line"><span class="comment">     5 使用窗口进行5s的计算</span></span><br><span class="line"><span class="comment">     6 sum出单词数量</span></span><br><span class="line"><span class="comment">     7 打印输出</span></span><br><span class="line"><span class="comment">     8 执行</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//1 创建一个流处理的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//2 构建socket source数据源</span></span><br><span class="line">    <span class="comment">//  socketTextStream参数：ip,port </span></span><br><span class="line">    <span class="comment">//  返回值类型是datastream</span></span><br><span class="line">    <span class="keyword">val</span> socketDs: <span class="type">DataStream</span>[<span class="type">String</span>] = env.socketTextStream(<span class="string">&quot;node1&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="comment">//这里可以在node1节点上使用nc -lk 9999命令开启一个服务器，并可以往里面写入数据</span></span><br><span class="line">    <span class="comment">//-l 用于指定netcat将处于侦听模式,指定该参数意味着nc被当作server，侦听并接受连接，而非向其它地址发起连接。</span></span><br><span class="line">    <span class="comment">//-k 表示持续打开连接</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//3 接收到的数据转为（单词，1）</span></span><br><span class="line">    <span class="keyword">val</span> tupleDs: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = socketDs.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map((_, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//4 对元组使用keyby分组（类似于批处理中的groupby）</span></span><br><span class="line">    <span class="keyword">val</span> keyedStream: <span class="type">KeyedStream</span>[(<span class="type">String</span>, <span class="type">Int</span>), <span class="type">Tuple</span>] = tupleDs.keyBy(<span class="number">0</span>) <span class="comment">//选择key value中的key</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//5 使用窗口进行5s的计算,每5s计算一次</span></span><br><span class="line">    <span class="keyword">val</span> windowStream: <span class="type">WindowedStream</span>[(<span class="type">String</span>, <span class="type">Int</span>), <span class="type">Tuple</span>, <span class="type">TimeWindow</span>] = keyedStream.timeWindow(<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//6 已经按照key进行了分组，因此这里对key value中的value进行求和</span></span><br><span class="line">    <span class="keyword">val</span> resDs: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = windowStream.sum(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//7 打印输出    </span></span><br><span class="line">    resDs.print()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//8 执行   流处理中一定要使用env.execute()来执行程序，不管有无print</span></span><br><span class="line">    env.execute()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="source">Source</h3>
<p>从数据源读取数据形成数据流DataStream。</p>
<h4 id="custom-source-自定义数据源">Custom Source 自定义数据源</h4>
<p>除了预定义的Source外，可以通过实现SourceFunction/ParallelSourceFunction/RichParallelSourceFunction等接口自定义数据源Source。</p>
<p>然后，可以在运行环境上指定使用的数据源，用StreamExecutionEnvironment.addSource( source )添加自定义数据源。</p>
<p>非并行数据源：SourceFunction, source不能设置大于1的并行度，效率会比较低。</p>
<p>并行数据源：ParallelSourceFunction，source可以设置大于1的并行度，效率更高。</p>
<p>富并行数据源：RichParallelSourceFunction，source可以设置大于1的并行度，此外还提供了open，close等方法。</p>
<h5 id="非并行数据源">非并行数据源</h5>
<p>此时addSource这个Operator的并行度（线程数量）只能设置为1，否则报错。</p>
<ul>
<li><p>创建一个class实现SourceFunction接口</p></li>
<li><p>重写写run方法，定义生产数据的业务逻辑，重写cancle方法定义取消发送数据</p></li>
<li><p>senv.addSource()添加自定义的source</p></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.<span class="type">SourceFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">自定义非并行数据源实现</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MySourceNoParalle</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1 创建一个流处理的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">// 2 添加自定义的数据源</span></span><br><span class="line">    <span class="keyword">val</span> myDs: <span class="type">DataStream</span>[<span class="type">Long</span>] = env.addSource(<span class="keyword">new</span> <span class="type">MyNoParalleSourceFunction</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//3 打印数据</span></span><br><span class="line">    myDs.print()</span><br><span class="line">    <span class="comment">//启动</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//SourceFunction泛型是自定义source的返回数据类型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyNoParalleSourceFunction</span> <span class="keyword">extends</span> <span class="title">SourceFunction</span>[<span class="type">Long</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> ele: <span class="type">Long</span> = <span class="number">0</span></span><br><span class="line">  <span class="keyword">var</span> isRunning = <span class="literal">true</span></span><br><span class="line">  <span class="comment">//发送数据，生产数据的方法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">Long</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">      ele += <span class="number">1</span></span><br><span class="line">      <span class="comment">//通过上下文对象发送数据</span></span><br><span class="line">      ctx.collect(ele)</span><br><span class="line">      <span class="comment">//降低发送速度</span></span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">1000</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 取消方法，取消是通过控制一个变量来影响run方法中的while循环，从而控制是否停止发送数据</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    isRunning = <span class="literal">false</span> <span class="comment">//取消发送数据</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="并行数据源">并行数据源</h5>
<p>ParallelSourceFunction</p>
<p>只需要将非并行自定义数据源实现的接口改为ParallelSourceFunction即可。</p>
<p>可以在Source Operator上设置大于1的并行度，从而并行读取数据。此时，发送数据是重复的，可以理解为有多个线程都在执行run这个方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.&#123;<span class="type">ParallelSourceFunction</span>, <span class="type">SourceFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">并行数据源实现</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyParallelSource</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1 创建一个流处理的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">// 2 添加自定义的数据源</span></span><br><span class="line">    <span class="keyword">val</span> myDs: <span class="type">DataStream</span>[<span class="type">Long</span>] = env.addSource(<span class="keyword">new</span> <span class="type">MyParalleSourceFunction</span>).setParallelism(<span class="number">3</span>)</span><br><span class="line">    <span class="comment">//3 打印数据</span></span><br><span class="line">    myDs.print()</span><br><span class="line">    <span class="comment">//启动</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//ParallelSourceFunction泛型是我们自定义source的返回数据类型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyParalleSourceFunction</span> <span class="keyword">extends</span> <span class="title">ParallelSourceFunction</span>[<span class="type">Long</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> ele: <span class="type">Long</span> = <span class="number">0</span></span><br><span class="line">  <span class="keyword">var</span> isRunning = <span class="literal">true</span></span><br><span class="line">  <span class="comment">//发送数据，生产数据的方法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">Long</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">      ele += <span class="number">1</span></span><br><span class="line">      <span class="comment">//通过上下文对象发送数据</span></span><br><span class="line">      ctx.collect(ele)</span><br><span class="line">      <span class="comment">//降低发送速度</span></span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">1000</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 取消方法，取消是通过控制一个变量来影响run方法中的while循环，从而控制是否停止发送数据</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    isRunning = <span class="literal">false</span> <span class="comment">//取消发送数据</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>RichParallelSourceFunction</p>
<p>富并行数据源，可以提供open，close等方法（如果操作数据库可以实现在open或者close打开关闭连接），也可以在其中获取到上下文执行更复杂的操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.&#123;<span class="type">RichParallelSourceFunction</span>, <span class="type">SourceFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">富并行数据源实现</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyRichParallelSource</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1 创建一个流处理的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">// 2 添加自定义的数据源</span></span><br><span class="line">    <span class="keyword">val</span> myDs: <span class="type">DataStream</span>[<span class="type">Long</span>] = env.addSource(<span class="keyword">new</span> <span class="type">MyRichParalleSourceFunction</span>).setParallelism(<span class="number">3</span>)</span><br><span class="line">    <span class="comment">//3 打印数据</span></span><br><span class="line">    myDs.print()</span><br><span class="line">    <span class="comment">//启动</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//SourceFunction泛型是我们自定义source的返回数据类型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRichParalleSourceFunction</span> <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>[<span class="type">Long</span>] </span>&#123;</span><br><span class="line">    </span><br><span class="line">  <span class="comment">//todo 初始化方法比如打开数据库连接等昂贵操作</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = <span class="keyword">super</span>.open(parameters)</span><br><span class="line">  <span class="comment">//todo 关闭连接</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = <span class="keyword">super</span>.close()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> ele: <span class="type">Long</span> = <span class="number">0</span></span><br><span class="line">  <span class="keyword">var</span> isRunning = <span class="literal">true</span></span><br><span class="line">  <span class="comment">//发送数据，生产数据的方法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">Long</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">      ele += <span class="number">1</span></span><br><span class="line">      <span class="comment">//通过上下文对象发送数据</span></span><br><span class="line">      ctx.collect(ele)</span><br><span class="line">      <span class="comment">//降低发送速度</span></span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">1000</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 取消方法，取消是通过控制一个变量来影响run方法中的while循环</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    isRunning = <span class="literal">false</span> <span class="comment">//取消发送数据</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>订单数据源样例</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">UUID</span></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.<span class="type">TimeUnit</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.&#123;<span class="type">RichParallelSourceFunction</span>, <span class="type">SourceFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">自定义数据源，练习 生成订单数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//订单信息(订单ID、用户ID、订单金额、时间戳)</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Order</span>(<span class="params">id: <span class="type">String</span>, userId: <span class="type">Int</span>, money: <span class="type">Long</span>, createTime: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">OrderCustomSource</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    1. 创建订单样例类</span></span><br><span class="line"><span class="comment">    2. 获取流处理环境</span></span><br><span class="line"><span class="comment">    3. 创建自定义数据源</span></span><br><span class="line"><span class="comment">       - 循环1000次</span></span><br><span class="line"><span class="comment">       - 随机构建订单信息</span></span><br><span class="line"><span class="comment">       - 上下文收集数据</span></span><br><span class="line"><span class="comment">       - 每隔一秒执行一次循环</span></span><br><span class="line"><span class="comment">    4. 打印数据</span></span><br><span class="line"><span class="comment">    5. 执行任务</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">      </span><br><span class="line">    <span class="comment">//1  获取流处理环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 2 加载自定义的order数据源,RichParallelSourceFunction泛型是生产的数据类型，order</span></span><br><span class="line">    <span class="keyword">val</span> orderDs: <span class="type">DataStream</span>[<span class="type">Order</span>] = env.addSource(<span class="keyword">new</span> <span class="type">RichParallelSourceFunction</span>[<span class="type">Order</span>] &#123;</span><br><span class="line">      <span class="keyword">var</span> isRunning = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">//2.1生成订单数据方法</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">Order</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//2.1.1 生成订单 业务逻辑</span></span><br><span class="line">        <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">          <span class="comment">//orderid</span></span><br><span class="line">          <span class="keyword">val</span> orderId = <span class="type">UUID</span>.randomUUID().toString</span><br><span class="line">          <span class="comment">//userid</span></span><br><span class="line">          <span class="keyword">val</span> userId = <span class="type">Random</span>.nextInt(<span class="number">3</span>)</span><br><span class="line">          <span class="comment">//money</span></span><br><span class="line">          <span class="keyword">val</span> money = <span class="type">Random</span>.nextInt(<span class="number">101</span>)</span><br><span class="line">          <span class="comment">//createTime</span></span><br><span class="line">          <span class="keyword">val</span> createTime = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">          ctx.collect(<span class="type">Order</span>(orderId, userId, money, createTime))</span><br><span class="line">          <span class="comment">//每隔一秒中执行一次</span></span><br><span class="line">          <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>.sleep(<span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">//2.2 取消数据的生成方法</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        isRunning = <span class="literal">false</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;) <span class="comment">//.setParallelism(1) 并行度设置可不用</span></span><br><span class="line">      </span><br><span class="line">    <span class="comment">//3 打印数据</span></span><br><span class="line">    orderDs.print()</span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 4 启动</span></span><br><span class="line">    env.execute()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="mysql数据源">MySQL数据源</h5>
<p>选择RichParallelSourceFunction接口作为要实现的接口，利用提供的open和close方法打开和关闭mysql的链接，实现链接的重用。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.&#123;<span class="type">Connection</span>, <span class="type">DriverManager</span>, <span class="type">PreparedStatement</span>, <span class="type">ResultSet</span>&#125;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.<span class="type">TimeUnit</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.&#123;<span class="type">RichParallelSourceFunction</span>, <span class="type">SourceFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">演示自定义并行数据源读取mysql</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//定义student 样例类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">Int</span>, name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">MysqlRichParallelSource</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1 创建一个流处理的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">// 2 添加自定义的数据源</span></span><br><span class="line">    <span class="keyword">val</span> stuDs: <span class="type">DataStream</span>[<span class="type">Student</span>] = env.addSource(<span class="keyword">new</span> <span class="type">MysqlRichParalleleSource</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//3 打印数据</span></span><br><span class="line">    stuDs.print()</span><br><span class="line">    <span class="comment">//4 启动</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2自定义mysql并行数据源</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlRichParalleleSource</span> <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>[<span class="type">Student</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> ps: <span class="type">PreparedStatement</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">var</span> connection: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.1 开启mysql连接</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//驱动方式</span></span><br><span class="line">    connection = <span class="type">DriverManager</span>.getConnection(<span class="string">&quot;jdbc:mysql://node1:3306/test&quot;</span>, <span class="string">&quot;root&quot;</span>, <span class="string">&quot;123456&quot;</span>)</span><br><span class="line">    <span class="comment">//准备sql语句查询表中全部数据</span></span><br><span class="line">    <span class="keyword">var</span> sql = <span class="string">&quot;select id ,name,age from t_student&quot;</span>;</span><br><span class="line">    <span class="comment">//准备执行语句对象</span></span><br><span class="line">    ps = connection.prepareStatement(sql)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//2.3 释放资源，关闭连接</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (connection != <span class="literal">null</span>) &#123;</span><br><span class="line">      connection.close()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (ps != <span class="literal">null</span>) ps.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> isRunning = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.2 读取mysql数据</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">Student</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">      <span class="comment">//读取mysql中的数据</span></span><br><span class="line">      <span class="keyword">val</span> result: <span class="type">ResultSet</span> = ps.executeQuery()</span><br><span class="line">      <span class="keyword">while</span> (result.next()) &#123;</span><br><span class="line">        <span class="keyword">val</span> userId = result.getInt(<span class="string">&quot;id&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> name = result.getString(<span class="string">&quot;name&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> age = result.getInt(<span class="string">&quot;age&quot;</span>)</span><br><span class="line">        <span class="comment">//收集并发送</span></span><br><span class="line">        ctx.collect(<span class="type">Student</span>(userId, name, age))</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">//休眠5s,执行一次</span></span><br><span class="line">      <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>.sleep(<span class="number">5</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//取消方法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    isRunning = <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="kafka数据源">Kafka数据源</h5>
<p>Flink提供了基于Kafka的数据源实现类FlinkKafkaConsumer011等 ，可以通过addSource添加该Kafka数据源。FlinkKafkaConsumer011等实现类最终是实现了RichParallelSourceFunction接口。</p>
<p><img src="/2021/05/09/Flink05_DataStreamAPI/Blog\source\_posts\Flink05_DataStreamAPI\0.png" style="zoom: 50%;"></p>
<p>FlinkKafkaConsumer等集成了Flink的检查点机制，可提供一次性处理语义。为实现这一目标，Flink并不完全依赖Kafka的消费者群体偏移跟踪，而是在内部实现了跟踪和检查这些偏移的机制。</p>
<ul>
<li><p>构造函数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> flinkKafkaConsumer: <span class="type">FlinkKafkaConsumer011</span>[<span class="type">String</span>] = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer011</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop)</span><br></pre></td></tr></table></figure>
<ol type="1">
<li><p>主题名称/主题名称列表</p></li>
<li><p>DeserializationSchema / KeyedDeserializationSchema</p>
<p>作用：对kafka里获取的二进制数据进行反序列化</p>
<p>反序列化Schema类型(接口)：DeserialzationSchema(只反序列化value)、KeyedDeserializationSchema(反序列化key和value)</p>
<p>FlinkKafkaConsumer需要知道如何将kafka中的二进制数据转换成Java/Scala对象，这两个接口定义了该转换模式，从kafka获取的每条消息都会通过其中的方法T deserialize(byte[] message)进行反序列化处理。接口源码为：</p>
<p><img src="/2021/05/09/Flink05_DataStreamAPI/Blog\source\_posts\Flink05_DataStreamAPI\1.png" style="zoom: 54%;"></p>
<p><img src="/2021/05/09/Flink05_DataStreamAPI/Blog\source\_posts\Flink05_DataStreamAPI\2.png" style="zoom:50%;"></p>
<p>常用的反序列化Schema：</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Schema</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SimpleStringSchema</td>
<td>可以将消息反序列化为字符串。当接收到消息并且反序列化失败的时候，会出现以下两种情况: <br>1) Flink从deserialize(..)方法中抛出异常，这会导致job的失败，然后job会重启；<br>2) 在deserialize(..) 方法出现失败的时候返回null，这会让Flink Kafka Consumer忽略这条消息。如果配置了checkpoint 为enable，由于consumer的失败容忍机制，失败的消息会被继续消费，因此还会继续失败，这就会导致job被不断自动重启</td>
</tr>
<tr class="even">
<td>JSONDeserializationSchema<br>JSONKeyValueDeserializationSchema</td>
<td>可以把序列化后的Json反序列化成ObjectNode，ObjectNode可以通过objectNode.get(“field”).as(Int/String/…)() 来访问指定的字段</td>
</tr>
<tr class="odd">
<td>TypeInformationSerializationSchema<br>TypeInformationKeyValueSerializationSchema</td>
<td>适合读写均是flink的场景。这会基于Flink的TypeInformation来创建Schema。这对于那些从Flink写入，又从Flink读出的数据是很有用的。这种Flink-Specific的反序列化会比其他通用的序列化方式带来更高的性能。</td>
</tr>
</tbody>
</table></li>
<li><p>Kafka消费者的属性。需要以下属性： “bootstrap.servers”（以逗号分隔的Kafka集群列表）</p>
<p>“group.id”消费者群组的ID</p>
<p>“zookeeper.connect”（逗号分隔的Zookeeper服务器列表）（仅Kafka 0.8需要）</p></li>
</ol></li>
<li><p>FlinkKafkaConsumer消费模式设置</p>
<p>这指定了了FlinkKafkaConsumer究竟是如何从Kafaka中消费数据的。</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody>
<tr class="odd">
<td>setStartFromEarliest</td>
<td>从队列头开始，最早的记录。内部的consumer递交到kafka/zk的偏移量将被忽略。</td>
</tr>
<tr class="even">
<td>setStartFromLatest</td>
<td>从队列尾开始，最新的记录 内部的consumer递交到kafka/zk的偏移量将被忽略。</td>
</tr>
<tr class="odd">
<td>setStartFromGroupOffsets</td>
<td>默认值，从当前消费组记录的偏移量开始，接着上次的偏移量消费。以consumer递交到kafka/zk中的偏移量为起始位置开始消费，group.id设置在consumer的properties里面。<br>如果没有找到记录的偏移量，则使用consumer的properties的auto.offset.reset设置的策略。</td>
</tr>
<tr class="even">
<td>setStartFromSpecificOffsets(Map&lt;TopicPartition, Long&gt;的参数)</td>
<td>从指定的具体位置开始消费。</td>
</tr>
<tr class="odd">
<td>setStartFromTimestamp(long)</td>
<td>从指定的时间戳开始消费 对于每个分区，时间戳大于或者等于指定时间戳的记录将用作起始位置，如果一个分区的最新时间早于时间戳，那么只需要从最新记录中读取该分区，在此模式下，kafka/zk中递交的偏移量将被忽略。<br>时间戳指的是kafka中消息自带的时间戳。</td>
</tr>
</tbody>
</table>
<ul>
<li><p>setStartFromEarliest()/ setStartFromLatest()</p>
<p>只会从最新或最老的地方开始消费。 注意：properties.setProperty("auto.offset.reset", "latest")和kafkaConsumer010.setStartFromLatest()不是一回事。</p></li>
<li><p>setStartFromGroupOffsets</p>
<p>任务从检查点重启，按照重启前的offset进行消费，如果直接重启不从检查点重启并且group.id不变，程序会按照上次提交的offset的位置继续消费。如果group.id改变了，则程序按照auto.offset.reset设置的属性进行消费。但是如果程序带有状态的算子，还是建议使用检查点重启。</p></li>
<li><p>setStartFromSpecificOffsets</p>
<p>从每个分区指定的偏移量读取。</p>
<p>注意 1：如果使用者需要读取在提供的偏移量映射中没有指定偏移量的分区，则它将回退到setStartFromGroupOffsets()该特定分区的默认组偏移行为。 注意 2：当作业从故障中自动恢复或使用保存点手动恢复时，这些起始位置配置方法不会影响起始位置。在恢复时，每个Kafka分区的起始位置由存储在保存点或检查点中的偏移量确定。</p>
<p>下面的示例将消费者配置为从主题“myTopic”的分区0,1和2的指定偏移量开始读取消息。偏移值应该是消费者应为每个分区读取的下一条记录的位置：</p>
<p><img src="/2021/05/09/Flink05_DataStreamAPI/Blog\source_posts\Flink05_DataStreamAPI\3.png"></p></li>
<li><p>setStartFromTimestamp</p>
<p>允许用户从指定的时间戳消费Kafka中的数据，指定一个时间戳即可，单位毫秒。从时间戳消费者忽略Zookeeper / Kafka代理中任何提交的组偏移量。消费者将查找时间戳大于或等于的最早偏移量到Kafka的特定时间戳。如果没有这样的偏移量，消费者将使用从kafka读取数据的最新偏移量。</p></li>
</ul></li>
<li><p>容错</p>
<p>启用Flink的检查点后，Flink Kafka Consumer将使用主题中的记录，并以一致的方式定期检查其所有Kafka偏移以及其他操作的状态。如果作业失败，Flink会将流式程序恢复到最新检查点的状态，从存储在检查点中的偏移量开始重新使用来自Kafka的消息数据。因此，设置检查点的间隔定义了程序在发生故障时最多可以返回多少。flink在使用kafka是要实现容错，需要在执行环境中启用拓扑的检查点：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.enableCheckpointing(<span class="number">1000</span>) <span class="comment">// checkpoint every 1000 msecs</span></span><br></pre></td></tr></table></figure>
<p>如果未启用检查点，Kafka（kafka 0.9 以前）使用者将定期向Zookeeper提交偏移量，kafka 0.9 以后提交到broker，都是将topic提交给__consumer_offsets函数来执行。</p>
<p>注意 ：如果flink任务 不通过检查点重启，而是直接重启（groupId不变），可能会丢失数据。</p>
<p>原因：kafka自动更新offset时，fetch到消息后就可以定期更新offset，无论是否消费成功。如果在kafka更新offset的间期内数据没有写入第三方介质，任务挂掉这部分数据就会丢失。</p></li>
<li><p>动态分区检测</p>
<p>随着业务增长数据量也会同步增长，将导致原有的 Kafka 分区数不满足数据写入所需的并发度，需要扩展 Kafka 的分区或者增加 Kafka 的 topic，这时实时处理框架能动态发现新增Topic分区并消费处理新增分区的数据。Flink则是通过创建一个线程，该线程会定期检测 Kafka 新增分区，然后将其添加到 KafkaFetcher 里。</p>
<p>Flink需要将 flink.partition-discovery.interval-millis 属性设置为大于 0 ，属性值为时间间隔，单位为毫秒。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//动态感知kafka主题分区的增加 单位毫秒</span></span><br><span class="line">properties.setProperty(<span class="string">&quot;flink.partition-discovery.interval-millis&quot;</span>, <span class="string">&quot;5000&quot;</span>);</span><br></pre></td></tr></table></figure></li>
<li><p>并行度</p>
<p>如果没有指定，Source Operator的个数与集群中的TaskManager的个数相等。如果手动设置，建议将总的Slot数量设为和Kafka的主题分区数相同。</p></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer011</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerConfig</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">验证flinkkafkaconsumer如何消费kafka中的数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestFlinkKafkaConsumer</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1 创建一个流处理的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">// 2 添加自定义的数据源</span></span><br><span class="line">    <span class="comment">//2.1 构建properties对象</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    <span class="comment">//kafka 集群地址</span></span><br><span class="line">    prop.setProperty(<span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span>, <span class="string">&quot;node1:9092,node2:9092&quot;</span>)</span><br><span class="line">    <span class="comment">//消费者组</span></span><br><span class="line">    prop.setProperty(<span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span>, <span class="string">&quot;flink&quot;</span>)</span><br><span class="line">    <span class="comment">//动态分区检测</span></span><br><span class="line">    prop.setProperty(<span class="string">&quot;flink.partition-discovery.interval-millis&quot;</span>, <span class="string">&quot;5000&quot;</span>)</span><br><span class="line">    <span class="comment">//设置kv的反序列化使用的类</span></span><br><span class="line">    prop.setProperty(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>)</span><br><span class="line">    prop.setProperty(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>)</span><br><span class="line">    <span class="comment">//设置默认消费的便宜量起始值</span></span><br><span class="line">    prop.setProperty(<span class="type">ConsumerConfig</span>.<span class="type">AUTO_OFFSET_RESET_CONFIG</span>, <span class="string">&quot;latest&quot;</span>) <span class="comment">//从最新处消费</span></span><br><span class="line">    <span class="comment">//定义topic</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">&quot;test&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//获得了kafkaconsumer对象</span></span><br><span class="line">    <span class="comment">//泛型限定了从kafka读取数据的类型</span></span><br><span class="line">    <span class="keyword">val</span> flinkKafkaConsumer: <span class="type">FlinkKafkaConsumer011</span>[<span class="type">String</span>] = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer011</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> kafkaDs: <span class="type">DataStream</span>[<span class="type">String</span>] = env.addSource(flinkKafkaConsumer)</span><br><span class="line">      </span><br><span class="line">    <span class="comment">//3 打印数据</span></span><br><span class="line">    kafkaDs.print()</span><br><span class="line">    <span class="comment">//4 启动</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可用的版本：</p>
<blockquote>
<p>Maven 依赖：flink-connector-kafka-0.11_2.11</p>
<p>Flink支持到的版本 ：1.4.0</p>
<p>生产者和消费者类的名称：FlinkKafkaConsumer011、FlinkKafkaProducer011</p>
<p>Kafka Version：0.11.x</p>
<p>描述：0.11.x Kafka不支持scala 2.10。此连接器支持Kafka事务性消息传递，为生产者提供一次语义。</p>
<p>​</p>
</blockquote>
<h4 id="socket-source">Socket Source</h4>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1 创建一个流处理的运行环境</span></span><br><span class="line"><span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    </span><br><span class="line"><span class="comment">//2 构建socket source数据源</span></span><br><span class="line"><span class="comment">//  socketTextStream参数：ip,port </span></span><br><span class="line"><span class="comment">//  返回值类型是datastream</span></span><br><span class="line"><span class="keyword">val</span> socketDs: <span class="type">DataStream</span>[<span class="type">String</span>] = env.socketTextStream(<span class="string">&quot;node1&quot;</span>, <span class="number">9999</span>)</span><br></pre></td></tr></table></figure>
<h4 id="集合source">集合Source</h4>
<h4 id="文件source">文件Source</h4>
<h3 id="transformation">Transformation</h3>
<p>对DataStream进行各种转换操作。</p>
<h4 id="keyby">keyby</h4>
<p>类似批处理中的group by算子，对数据流按照指定规则进行分区。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">keyby的实现单词统计</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KeyByDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1 创建一个流处理的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">// 2 加载socketstream</span></span><br><span class="line">    <span class="keyword">val</span> socketDs: <span class="type">DataStream</span>[<span class="type">String</span>] = env.socketTextStream(<span class="string">&quot;node1&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="comment">//3 对接收到的数据切分压平转成单词，1的元组</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOneDs: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = socketDs.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(_ -&gt; <span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 4 按照单词分组</span></span><br><span class="line"><span class="comment">//    wordAndOneDs.keyBy(_._1).sum(1).print()</span></span><br><span class="line">    wordAndOneDs.keyBy(<span class="number">0</span>).sum(<span class="number">1</span>).print()</span><br><span class="line">    <span class="comment">//5 启动</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="connect">connect</h4>
<p>将两个DataStream合并为一个流，数据类型可以不同。批处理中union算子必须要求数据类型一致才能union。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.concurrent.<span class="type">TimeUnit</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.<span class="type">SourceFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">ConnectedStreams</span>, <span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">flink中connect的用法，把两个数据流连接到一起</span></span><br><span class="line"><span class="comment">需求：</span></span><br><span class="line"><span class="comment">创建两个流，一个产生数值，一个产生字符串数据</span></span><br><span class="line"><span class="comment">使用connect连接两个流</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ConnectDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1 创建一个流处理的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">// 2 加载source</span></span><br><span class="line">    <span class="keyword">val</span> numDs: <span class="type">DataStream</span>[<span class="type">Long</span>] = env.addSource(<span class="keyword">new</span> <span class="type">MyNumberSource</span>)</span><br><span class="line">    <span class="keyword">val</span> strDs = env.addSource(<span class="keyword">new</span> <span class="type">MyStrSource</span>)</span><br><span class="line">    <span class="comment">// 3 使用connect进行两个连接操作</span></span><br><span class="line">     <span class="keyword">val</span> connectedDs: <span class="type">ConnectedStreams</span>[<span class="type">Long</span>, <span class="type">String</span>] = numDs.connect(strDs)</span><br><span class="line">    <span class="comment">//传递两个函数，分别处理数据</span></span><br><span class="line">    <span class="keyword">val</span> resDs: <span class="type">DataStream</span>[<span class="type">String</span>] = connectedDs.map(l=&gt;<span class="string">&quot;long&quot;</span>+l, s=&gt;<span class="string">&quot;string&quot;</span>+s)</span><br><span class="line">    <span class="comment">//connect意义在哪里呢？只是把两个合并为一个，但是处理业务逻辑都是按照自己的方法处理？</span></span><br><span class="line">    <span class="comment">//connect之后两条流可以共享状态数据</span></span><br><span class="line">    resDs.print()</span><br><span class="line">    <span class="comment">//5 启动</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//自定义产生递增的数字 第一个数据源</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyNumberSource</span> <span class="keyword">extends</span> <span class="title">SourceFunction</span>[<span class="type">Long</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">var</span> flag=<span class="literal">true</span></span><br><span class="line">  <span class="keyword">var</span> num=<span class="number">1</span>L</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">Long</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">while</span>(flag)&#123;</span><br><span class="line">      num +=<span class="number">1</span></span><br><span class="line">      ctx.collect(num)</span><br><span class="line">      <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>.sleep(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    flag=<span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义产生从1开始递增字符串</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyStrSource</span> <span class="keyword">extends</span> <span class="title">SourceFunction</span>[<span class="type">String</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">var</span> flag=<span class="literal">true</span></span><br><span class="line">  <span class="keyword">var</span> num=<span class="number">1</span>L</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">while</span>(flag)&#123;</span><br><span class="line">      num +=<span class="number">1</span></span><br><span class="line">      ctx.collect(<span class="string">&quot;str&quot;</span>+num)</span><br><span class="line">      <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>.sleep(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    flag=<span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="splitselect">split+select</h4>
<p>实现对数据流的切分，使用split切分流，通过select获取到切分之后的流。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">SplitStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">示例</span></span><br><span class="line"><span class="comment">加载本地集合(1,2,3,4,5,6), 使用split进行数据分流,分为奇数和偶数. 并打印奇数结果</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SplitSelectDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1 创建一个流处理的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//2 加载source</span></span><br><span class="line">    <span class="keyword">val</span> numDs: <span class="type">DataStream</span>[<span class="type">Int</span>] = env.fromCollection(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 3 转换 使用split把数据流中的数据分为奇数和偶数</span></span><br><span class="line">    <span class="keyword">val</span> splitStream: <span class="type">SplitStream</span>[<span class="type">Int</span>] = numDs.split(</span><br><span class="line">      item =&gt; &#123;</span><br><span class="line">        <span class="comment">//模以2</span></span><br><span class="line">        <span class="keyword">var</span> res = item % <span class="number">2</span></span><br><span class="line">        <span class="comment">//模式匹配的方式</span></span><br><span class="line">        res <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="number">0</span> =&gt; <span class="type">List</span>(<span class="string">&quot;even&quot;</span>) <span class="comment">//偶数  even与odd只是名称，代表数据流的名称，但是必须放在list集合</span></span><br><span class="line">          <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="type">List</span>(<span class="string">&quot;odd&quot;</span>) <span class="comment">//奇数</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 4 从splitStream中获取奇数流和偶数流</span></span><br><span class="line">    <span class="keyword">val</span> evenDs: <span class="type">DataStream</span>[<span class="type">Int</span>] = splitStream.select(<span class="string">&quot;even&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> oddDs = splitStream.select(<span class="string">&quot;odd&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> allDs: <span class="type">DataStream</span>[<span class="type">Int</span>] = splitStream.select(<span class="string">&quot;even&quot;</span>,<span class="string">&quot;odd&quot;</span>)</span><br><span class="line">    <span class="comment">// 5打印结果</span></span><br><span class="line">    <span class="comment">//evenDs.print()</span></span><br><span class="line">    <span class="comment">//oddDs.print()</span></span><br><span class="line">    allDs.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5启动程序</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="sink">Sink</h3>
<h4 id="mysql-sink">MySQL Sink</h4>
<p>大致流程：</p>
<p>1、创建class实现RichSinkFunction接口，并通过addSink方法添加给DataStream实现数据的保存。</p>
<p>2、重写invoke方法，设置SQL语句中的数值，执行真正写入逻辑的方法。</p>
<p>3、利用open和close方法实现对数据库连接的管理</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.&#123;<span class="type">Connection</span>, <span class="type">DriverManager</span>, <span class="type">PreparedStatement</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.&#123;<span class="type">RichSinkFunction</span>, <span class="type">SinkFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">flink程序计算结果保存到mysql中</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//定义student case class</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">Int</span>, name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">SinkToMysqlDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    读取数据然后直接写入mysql,需要自己实现mysql sinkfunction</span></span><br><span class="line"><span class="comment">    自定义class实现RichSinkFunction重写open,invoke,close方法</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//1 创建一个流处理的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">// 2 加载source</span></span><br><span class="line">    <span class="keyword">val</span> stuDs: <span class="type">DataStream</span>[<span class="type">Student</span>] = env.fromElements(<span class="type">Student</span>(<span class="number">0</span>, <span class="string">&quot;tony&quot;</span>, <span class="number">18</span>))</span><br><span class="line">    <span class="comment">// 3 直接写出到mysql</span></span><br><span class="line">    stuDs.addSink(<span class="keyword">new</span> <span class="type">MySqlSinkFunction</span>)</span><br><span class="line">    <span class="comment">// 4 执行</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//准备自定义mysql sinkfunciton</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySqlSinkFunction</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>[<span class="type">Student</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> ps: <span class="type">PreparedStatement</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">var</span> connection: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">  <span class="comment">// 3.1 打开连接</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 3.1.1驱动方式</span></span><br><span class="line">    connection = <span class="type">DriverManager</span>.getConnection(<span class="string">&quot;jdbc:mysql://node1:3306/test&quot;</span>, <span class="string">&quot;root&quot;</span>, <span class="string">&quot;123456&quot;</span>)</span><br><span class="line">    <span class="comment">//3.1.2准备sql语句插入数据到mysql表中</span></span><br><span class="line">    <span class="keyword">var</span> sql = <span class="string">&quot;insert into t_student(name,age) values(?,?)&quot;</span>;</span><br><span class="line">    <span class="comment">//3.1.3准备执行语句对象</span></span><br><span class="line">    ps = connection.prepareStatement(sql)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//关闭连接</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (connection != <span class="literal">null</span>) &#123;</span><br><span class="line">      connection.close()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (ps != <span class="literal">null</span>) ps.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3.2 这个方法负责写入数据到mysql中,value就是上游datastream传入需要写入mysql的数据</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">invoke</span></span>(value: <span class="type">Student</span>, context: <span class="type">SinkFunction</span>.<span class="type">Context</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 3.2.1设置参数</span></span><br><span class="line">    ps.setString(<span class="number">1</span>, value.name)</span><br><span class="line">    ps.setInt(<span class="number">2</span>, value.age)</span><br><span class="line">    <span class="comment">//3.2.2执行插入动作</span></span><br><span class="line">    ps.executeUpdate()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="kafka-sink">Kafka Sink</h4>
<p>利用Flink提供的FlinkKafkaProducer这个类，实现写出数据到Kafka中，此时Flink作为生产者，Kafka作为消费者。</p>
<p>FlinkKafkaProducer的构造参数需要指定序列化数据的Schema，下面的案例使用keyedSerializationWrapper：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaProducer011</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.util.serialization.<span class="type">KeyedSerializationSchemaWrapper</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.<span class="type">ProducerConfig</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">flink程序计算结果保存到kafka</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//定义student case class</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">Int</span>, name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">SinkToKafkaDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    flink读取数据然后把数据写入kafka中</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//1 创建一个流处理的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//2 加载source</span></span><br><span class="line">    <span class="keyword">val</span> stuDs: <span class="type">DataStream</span>[<span class="type">Student</span>] = env.fromElements(<span class="type">Student</span>(<span class="number">0</span>, <span class="string">&quot;tony&quot;</span>, <span class="number">18</span>))</span><br><span class="line">      </span><br><span class="line">    <span class="comment">//3 直接使用flinkkafkaproducer来生产数据到kafka</span></span><br><span class="line">    <span class="comment">//3.1 准备一个flinkkafkaproducer对象</span></span><br><span class="line">    <span class="comment">//param1 Kafaka中的主题名称</span></span><br><span class="line">    <span class="keyword">var</span> topic=<span class="string">&quot;test&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//param2 指定序列化所使用的Schema</span></span><br><span class="line">    <span class="keyword">val</span> keyedSerializationWrapper: <span class="type">KeyedSerializationSchemaWrapper</span>[<span class="type">String</span>] =</span><br><span class="line">      <span class="keyword">new</span> <span class="type">KeyedSerializationSchemaWrapper</span>(<span class="keyword">new</span> <span class="type">SimpleStringSchema</span>())</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//param3 指定属性，Kafka集群地址</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="type">ProducerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span>,<span class="string">&quot;node1:9092,node2:9092&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//泛型指定写入kafka的数据类型      </span></span><br><span class="line">    <span class="keyword">val</span> flinkKafkaProducer: <span class="type">FlinkKafkaProducer011</span>[<span class="type">String</span>] = <span class="keyword">new</span> <span class="type">FlinkKafkaProducer011</span>[<span class="type">String</span>](</span><br><span class="line">      topic,keyedSerializationWrapper,prop)</span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 4 sink 操作</span></span><br><span class="line">    stuDs.map(_.toString).addSink(flinkKafkaProducer)</span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 5 执行</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="redis-sink">Redis Sink</h4>
<p>借助Flink提供的Redis Sink我们可以方便的把数据写入Redis中。使用Redis Sink需要提供两项内容：</p>
<p>1、连接Redis的配置文件</p>
<p>2、提供一个RedisMapper接口的实现类的对象，其中重写三个方法，分别定义了操作的数据结构，写入的key和value。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.<span class="type">RedisSink</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.config.<span class="type">FlinkJedisPoolConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.mapper.&#123;<span class="type">RedisCommand</span>, <span class="type">RedisCommandDescription</span>, <span class="type">RedisMapper</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">flink程序计算结果保存到redis，使用flink提供的redissink</span></span><br><span class="line"><span class="comment">从socket接收数据然后计算出单词的次数，最终使用redissink写数据到redis中</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SinkToRedisDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1 创建一个流处理的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//2 加载socket数据，</span></span><br><span class="line">    <span class="keyword">val</span> socketDs: <span class="type">DataStream</span>[<span class="type">String</span>] = env.socketTextStream(<span class="string">&quot;node1&quot;</span>,<span class="number">9999</span>)</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    单词计数的逻辑</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//3 转换</span></span><br><span class="line">    <span class="keyword">val</span> resDs: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = socketDs.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(_ -&gt;<span class="number">1</span>).keyBy(<span class="number">0</span>).sum(<span class="number">1</span>)</span><br><span class="line">      </span><br><span class="line">    <span class="comment">//4 Sink 操作 使用Redis Sink</span></span><br><span class="line">    <span class="comment">//4.1 redissink的构造：1、需要redis配置文件（连接信息），2、RedisMapper接口的实现类的对象</span></span><br><span class="line">    <span class="comment">//4.1.1 jedisconfig</span></span><br><span class="line">    <span class="keyword">val</span> config: <span class="type">FlinkJedisPoolConfig</span> = <span class="keyword">new</span> <span class="type">FlinkJedisPoolConfig</span>.<span class="type">Builder</span>().setHost(<span class="string">&quot;node2&quot;</span>).setPort(<span class="number">6379</span>).build()</span><br><span class="line"></span><br><span class="line">    resDs.addSink(<span class="keyword">new</span> <span class="type">RedisSink</span>[(<span class="type">String</span>, <span class="type">Int</span>)](config,<span class="keyword">new</span> <span class="type">MyRedisMapper</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5 执行</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//4.1.2 RedisMapper的对象，泛型就是DataStream的数据类型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRedisMapper</span> <span class="keyword">extends</span> <span class="title">RedisMapper</span>[(<span class="type">String</span>, <span class="type">Int</span>)]</span>&#123;</span><br><span class="line">  <span class="comment">//获取命令描述器，确定数据结构,使用hash结构</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getCommandDescription</span></span>: <span class="type">RedisCommandDescription</span> = &#123;</span><br><span class="line">    <span class="comment">//指定使用hset命令，并提供hash结构的第一个key</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">RedisCommandDescription</span>(<span class="type">RedisCommand</span>.<span class="type">HSET</span>,<span class="string">&quot;REDISSINK&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//指定key</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getKeyFromData</span></span>(data: (<span class="type">String</span>, <span class="type">Int</span>)): <span class="type">String</span> = &#123;</span><br><span class="line">    data._1</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//指定value</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValueFromData</span></span>(data: (<span class="type">String</span>, <span class="type">Int</span>)): <span class="type">String</span> = &#123;</span><br><span class="line">    data._2.toString</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>注意导入Maven依赖：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;flink-connector-redis_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">	&lt;version&gt;1.1.5&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>
<h3 id="参考资料">参考资料</h3>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Flink/" rel="tag"># Flink</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/05/09/Flink04_DatasetAPI/" rel="prev" title="04 Flink Dataset">
                  <i class="fa fa-chevron-left"></i> 04 Flink Dataset
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/05/13/Flink06_WindowAndTime/" rel="next" title="06 Flink四大基石：窗口与时间">
                  06 Flink四大基石：窗口与时间 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">BIOINSu</span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/local-search.js"></script>






  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
