<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2021/01/28/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<a id="more"></a>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<figure>
<img src="/2021/01/28/hello-world/0.png" alt="hello"><figcaption aria-hidden="true">hello</figcaption>
</figure>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <tags>
        <tag>Testing</tag>
        <tag>Another Tag</tag>
      </tags>
  </entry>
  <entry>
    <title>Java中char、int、String相互转换</title>
    <url>/2021/04/01/Java%E4%B8%ADchar,int,String%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2/</url>
    <content><![CDATA[<h4 id="char转int">char转int</h4>
<p>在Java中用2个字节，即16位，来表示一个char。</p>
<a id="more"></a>
<ul>
<li><p>当char为数字时</p>
<p>即'0'、'1'、'2'、'3'...、'9'时，则直接减去'0'可以得到对应的数字。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">char</span> nine = <span class="string">&#x27;9&#x27;</span>;</span><br><span class="line"><span class="keyword">int</span> num = <span class="string">&#x27;9&#x27;</span> - <span class="string">&#x27;0&#x27;</span>; <span class="comment">//num即得到9</span></span><br></pre></td></tr></table></figure></li>
<li><p>当char为其他字符时</p>
<p>使用强制类型转换即可获得该字符表示的ASCII 编码。</p>
<p>字符<strong>0-9</strong>的ASCII码：48-57；</p>
<p>大写字母<strong>A-Z</strong>的ASCII码为：65-90；</p>
<p>小写字母<strong>a-z</strong>的ASCII码为：97-122；</p>
<p><strong>空字符</strong>的ASCII码为0；</p>
<p><strong>换行符</strong>的ASCII码为10。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">char</span> tmp = <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line"><span class="keyword">int</span> num = (<span class="keyword">int</span>)tmp; <span class="comment">//num此时为&#x27;a&#x27;的ASCII 编码97</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="int转char">int转char</h4>
<ul>
<li><p>当int为两位以上的数字时，使用强制类型转换将变成该数字代表的ASCII码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> num = <span class="number">97</span>;</span><br><span class="line"><span class="keyword">char</span> ca = (<span class="keyword">char</span>)num; <span class="comment">//ca此时就代表了小写字母&#x27;a&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li><p>当int为0到9的数字时，利用ASCII码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> num = <span class="number">9</span>;</span><br><span class="line"><span class="keyword">char</span> ca = (<span class="keyword">char</span>)(num+<span class="number">48</span>) <span class="comment">//ca此时就代表了数字&#x27;9&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li><p>先将int转为String，再转成char，可以将数字拆开</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> num = <span class="number">4596</span>;</span><br><span class="line">String str = String.valueOf(num);</span><br><span class="line"><span class="keyword">char</span>[] array = str.toCharArray();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; array.length; i++) &#123;</span><br><span class="line">	System.out.print(array[i]+<span class="string">&quot; &quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">4 5 9 6</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="char转string">char转String</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String str = String.valueOf(<span class="string">&#x27;a&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//char数组转String</span></span><br><span class="line"><span class="keyword">char</span>[] s=&#123;<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;G&#x27;</span>,<span class="string">&#x27;C&#x27;</span>,<span class="string">&#x27;T&#x27;</span>&#125;;</span><br><span class="line">String st=String.valueOf(s);</span><br></pre></td></tr></table></figure>
<h4 id="string转char">String转char</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String str = <span class="string">&quot;Hello World&quot;</span>;</span><br><span class="line"><span class="keyword">char</span>[] array = str.toCharArray();</span><br></pre></td></tr></table></figure>
<h4 id="string转int">String转int</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String str = <span class="string">&quot;6666&quot;</span>;</span><br><span class="line"><span class="keyword">int</span> num = Integer.valueOf(str);</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">当str非数字时报出运行时异常</span></span><br><span class="line"><span class="comment">java.lang.NumberFormatException</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//Stirng数组转int数组，利用Java 8中的stream方法</span></span><br><span class="line">String[] strings = &#123;<span class="string">&quot;1&quot;</span>, <span class="string">&quot;2&quot;</span>, <span class="string">&quot;3&quot;</span>&#125;;</span><br><span class="line"><span class="keyword">int</span>[] array = Arrays.stream(strings).mapToInt(Integer::parseInt).toArray();</span><br></pre></td></tr></table></figure>
<h4 id="int转string">int转String</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> num = <span class="number">6666</span>;</span><br><span class="line">String str = String.valueOf(num);</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java基础</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark闭包、累加器和广播变量</title>
    <url>/2021/04/11/Spark%E9%97%AD%E5%8C%85%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/</url>
    <content><![CDATA[<h3 id="总结">总结</h3>
<ul>
<li><p>闭包</p>
<p>闭包<strong>就是一个携带了外部作用域的函数</strong>，当它需要被分发到各个节点上执行的时候，需要外部作用域所在的对象是可序列化的。</p>
<p>使用这种外部变量的形式，会造成每个Task中都存在一个该变量的副本。</p></li>
</ul>
<a id="more"></a>
<ul>
<li><p>广播变量</p>
<p>广播变量是一种变量，广播变量允许开发者将一个 <code>Read-Only</code> 的变量<strong>缓存到集群中每个节点中</strong>，而不是传递给每一个 Task 一个副本，每个Executor中只会存在一个副本。</p>
<ul>
<li><p>解决了外部变量在每个Task中都存在副本的问题，每个Executor只会存在一份广播变量的副本，节省空间。</p></li>
<li><p>在Driver端可以修改广播变量的值，<strong>在Executor端无法修改广播变量的值。</strong></p></li>
<li><p>广播变量只能在Driver端定义，不能在Executor端定义。</p></li>
<li><p>不能将RDD广播，因为RDD是不存储数据的。可以将RDD的结果广播出去。</p></li>
</ul></li>
<li><p>累加器</p>
<p>累加器是一个<strong>支持添加操作的分布式变量</strong>，可以在分布式环境下保持一致性。</p>
<ul>
<li>累加器在Driver端定义赋初始值，累加器只能在Driver端读取最后的值，在Excutor端更新。</li>
<li>在Driver端可以修改广播变量的值，<strong>在Executor端可以修改累加器的值。</strong></li>
</ul></li>
</ul>
<h3 id="闭包">闭包</h3>
<p>如果一个函数<strong>携带了外部的作用域</strong>，这种<strong>函数称为闭包</strong>。</p>
<p>在Scala中，函数是一个对象，继承自FunctionN。</p>
<h4 id="问题引出">问题引出</h4>
<p>在spark中实现统计List(1,2,3)的和。如果使用下面的代码，程序打印的结果不是6，而是0。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]):<span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;test&quot;</span>);</span><br><span class="line">      <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">      <span class="keyword">var</span> counter = <span class="number">0</span></span><br><span class="line">      <span class="comment">//warn: don&#x27;t do this</span></span><br><span class="line">      rdd.foreach(x =&gt; counter += x)</span><br><span class="line">      println(<span class="string">&quot;Counter value: &quot;</span>+counter) <span class="comment">//打印结果为0</span></span><br><span class="line"></span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>问题分析： counter是在foreach函数外部定义的，也就是在driver程序中定义，而foreach函数是属于rdd对象的，rdd函数的执行位置是各个worker节点（或者说worker进程），main函数是在driver节点上（或者说driver进程上）执行的。所以当counter变量在driver中定义，被在rdd中使用的时候，出现了变量的跨域问题，也就是闭包问题。</p>
<p>问题解释： 对于上面程序中的counter变量，由于在main函数和在rdd对象中foreach函数是属于不同作用域的，所以，传进foreach中的counter是一个副本，初始值都为0。foreach中叠加的是counter的副本，不管副本如何变化，都不会影响到main函数中的counter，所以最终打印出来的counter为0。</p>
<h4 id="解决方案">解决方案</h4>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="keyword">new</span> <span class="type">SparkConf</span>())</span><br><span class="line">    <span class="keyword">val</span> newRDD = sc.textFile(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">    newRDD.map(data =&gt; &#123;</span><br><span class="line">      <span class="comment">//do something</span></span><br><span class="line">      println(data.toString)</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果在RDD的函数中调用了在函数外部定义的对象，这些对象需要通过网络从driver所在节点传给其他的worker节点，所以<strong>要求这些类是可序列化的</strong>，比如在Java或者Scala中实现Serializable类。Worker节点接收到程序之后，在Spark资源管理器的指挥下运行RDD程序。不同Worker节点之间的运行操作是并行的。</p>
<p><img src="/2021/04/11/Spark%E9%97%AD%E5%8C%85%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/0.png" style="zoom:67%;"></p>
<p>除了用外部定义的对象来实现类似的功能，Spark还另外提供了两种机制，分别是<strong>Broadcast</strong>和<strong>Accumulator</strong>。</p>
<p>相比于外部变量的方式，在一定场景下使用Broadcast比较有优势，因为所广播的数据在每一个Worker节点上面只存一个副本，而在Spark算子中使用到的外部变量会在每一个用到它的Task中保存一个副本，即使这些task在同一个节点上面。所以当数据量比较大的时候，建议使用广播而不是外部变量。</p>
<h3 id="广播变量">广播变量</h3>
<p>广播变量是一种变量，广播变量允许开发者将一个 <code>Read-Only</code> 的变量缓存到集群中每个节点中, 而不是传递给每一个 Task 一个副本，每个Executor中只会存在一个副本。</p>
<p>变量一旦被定义为一个广播变量，那么这个变量只能读，不能修改。</p>
<p><img src="/2021/04/11/Spark%E9%97%AD%E5%8C%85%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/2.png" style="zoom:50%;"></p>
<h4 id="广播变量的使用">广播变量的使用</h4>
<h5 id="创建广播变量">创建广播变量</h5>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> b = sc.broadcast(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>如果 Log 级别为 DEBUG 的时候, 会打印如下信息</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">DEBUG BlockManager: Put block broadcast_0 locally took  430 ms</span><br><span class="line">DEBUG BlockManager: Putting block broadcast_0 without replication took  431 ms</span><br><span class="line">DEBUG BlockManager: Told master about block broadcast_0_piece0</span><br><span class="line">DEBUG BlockManager: Put block broadcast_0_piece0 locally took  4 ms</span><br><span class="line">DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took  4 ms</span><br></pre></td></tr></table></figure>
<h5 id="使用-value-获取数据">使用 <code>value</code> 获取数据</h5>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">b.value</span><br></pre></td></tr></table></figure>
<p>获取数据的时候会打印如下信息</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">DEBUG BlockManager: Getting local block broadcast_0</span><br><span class="line">DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)</span><br></pre></td></tr></table></figure>
<p>使用 <code>value</code> 方法的注意点</p>
<p>方法签名 <code>value: T</code></p>
<p>在destroy之前才能使用value：在 <code>value</code> 方法内部会确保使用获取数据的时候, 变量必须是可用状态, 所以必须在变量被 <code>destroy</code> 之前使用 <code>value</code> 方法, 如果使用 <code>value</code> 时变量已经失效, 则会报出以下错误：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">org.apache.spark.SparkException: Attempted to use Broadcast(0) after it was destroyed (destroy at &lt;console&gt;:27)</span><br><span class="line">  at org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144)</span><br><span class="line">  at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:69)</span><br><span class="line">  ... 48 elided</span><br></pre></td></tr></table></figure>
<h5 id="使用-unpersist-删除数据">使用 <code>unpersist</code> 删除数据</h5>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">b.unpersist</span><br></pre></td></tr></table></figure>
<h5 id="使用-destroy-销毁变量">使用 <code>destroy</code> 销毁变量,</h5>
<p>释放内存空间</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">b.destroy</span><br></pre></td></tr></table></figure>
<p>销毁以后, 会打印如下信息</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">DEBUG BlockManager: Removing broadcast 0</span><br><span class="line">DEBUG BlockManager: Removing block broadcast_0_piece0</span><br><span class="line">DEBUG BlockManager: Told master about block broadcast_0_piece0</span><br><span class="line">DEBUG BlockManager: Removing block broadcast_0</span><br></pre></td></tr></table></figure>
<p>使用 <code>destroy</code> 方法的注意点</p>
<p>方法签名 <code>destroy(): Unit</code></p>
<p>不要多次删除广播变量：<code>destroy</code> 方法会移除广播变量, 彻底销毁掉, 但是如果你试图多次 <code>destroy</code> 广播变量, 则会报出以下错误</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">org.apache.spark.SparkException: Attempted to use Broadcast(0) after it was destroyed (destroy at &lt;console&gt;:27)</span><br><span class="line">  at org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144)</span><br><span class="line">  at org.apache.spark.broadcast.Broadcast.destroy(Broadcast.scala:107)</span><br><span class="line">  at org.apache.spark.broadcast.Broadcast.destroy(Broadcast.scala:98)</span><br><span class="line">  ... 48 elided</span><br></pre></td></tr></table></figure>
<h4 id="广播变量的使用场景">广播变量的使用场景</h4>
<p>正常情况下使用 Task 拉取数据的时候, 会将数据拷贝到 Executor 中多次, 但是使用广播变量的时候只会复制一份数据到 Executor 中, 所以在两种情况下特别适合使用广播变量</p>
<ul>
<li>一个 Executor 中有多个 Task 的时候</li>
<li>一个变量比较大的时候</li>
<li>大RDD和小RDD执行join操作时。当一个 RDD 很大并且还需要和另外一个 RDD 执行 <code>join</code> 的时候, 可以将较小的 RDD 广播出去, 然后使用大的 RDD 在算子 <code>map</code> 中直接 <code>join</code>, 从而实现在 Map 端 <code>join</code>。</li>
</ul>
<h3 id="累加器">累加器</h3>
<h4 id="通用累加器">通用累加器</h4>
<p>Accumulators(累加器) 是一个只支持 added(添加) 的分布式变量, 可以在分布式环境下保持一致性, 并且能够做到高效的并发。</p>
<p><img src="/2021/04/11/Spark%E9%97%AD%E5%8C%85%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/1.png" style="zoom:50%;"></p>
<ul>
<li><p>Accumulator 是支持并发并行的, 在任何地方都可以通过 add 来修改数值, 无论是 Driver 还是 Executor</p></li>
<li><p>只能在 Driver 中才能调用 value 来获取数值</p></li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;ip_ana&quot;</span>).setMaster(<span class="string">&quot;local[6]&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(config)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> counter = sc.longAccumulator(<span class="string">&quot;counter&quot;</span>)</span><br><span class="line"></span><br><span class="line">sc.parallelize(<span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">  .foreach(counter.add(_))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 运行结果: 15</span></span><br><span class="line">println(counter.value)</span><br></pre></td></tr></table></figure>
<ul>
<li>累加器能保证在 Spark 任务出现问题被重启的时候不会出现重复计算</li>
<li>累加器只有在 Action 执行的时候才会被触发</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;ip_ana&quot;</span>).setMaster(<span class="string">&quot;local[6]&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(config)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> counter = sc.longAccumulator(<span class="string">&quot;counter&quot;</span>)</span><br><span class="line"></span><br><span class="line">sc.parallelize(<span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">  .map(counter.add(_)) <span class="comment">// 这个地方不是 Action, 而是一个 Transformation</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 运行结果是 0</span></span><br><span class="line">println(counter.value)</span><br></pre></td></tr></table></figure>
<h4 id="自定义累加器">自定义累加器</h4>
<p>可以通过自定义累加器来实现更多类型的累加器, 累加器的作用远远不只是累加, 比如可以实现一个累加器, 用于向里面添加一些运行信息。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//继承AccumulatorV2这个类，传入String类型，返回Set[String]类型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InfoAccumulator</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">String</span>, <span class="type">Set</span>[<span class="type">String</span>]] </span>&#123;</span><br><span class="line">  <span class="comment">//定义一个可变长度的Set，即 mutable.Set[String]  </span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> infos: mutable.<span class="type">Set</span>[<span class="type">String</span>] = mutable.<span class="type">Set</span>()</span><br><span class="line"></span><br><span class="line">  <span class="comment">//判断累加器对象是否为空，这里就是判断这个Set是否为空</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    infos.isEmpty</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//返回一个拷贝的累加器</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">String</span>, <span class="type">Set</span>[<span class="type">String</span>]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> newAccumulator = <span class="keyword">new</span> <span class="type">InfoAccumulator</span>()</span><br><span class="line">    infos.synchronized &#123; <span class="comment">//使用synchronized关键字保证线程安全</span></span><br><span class="line">      newAccumulator.infos ++= infos <span class="comment">//把一个Set添加到另一个Set中使用++=</span></span><br><span class="line">    &#125;</span><br><span class="line">    newAccumulator</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//清空累加器</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    infos.clear()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//往累加器中添加元素</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    infos += v</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//累加器在进行累加的时候，可能每个分布式节点都有一个实例</span></span><br><span class="line">  <span class="comment">//在最后的Driver中进行合并，通过调用这个merge方法把所有的实例内容合并起来，</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">String</span>, <span class="type">Set</span>[<span class="type">String</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    infos ++= other.value <span class="comment">//这里的value就是下面这个方法返回的</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//提供给外部累加结果，注意返回一个不可变的类型，防止外部进行修改</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: <span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    infos.toSet</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//测试案例</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accumulator2</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;ip_ana&quot;</span>).setMaster(<span class="string">&quot;local[6]&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(config)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> infoAccumulator = <span class="keyword">new</span> <span class="type">InfoAccumulator</span>()</span><br><span class="line">  sc.register(infoAccumulator, <span class="string">&quot;infos&quot;</span>)</span><br><span class="line"></span><br><span class="line">  sc.parallelize(<span class="type">Seq</span>(<span class="string">&quot;1&quot;</span>, <span class="string">&quot;2&quot;</span>, <span class="string">&quot;3&quot;</span>))</span><br><span class="line">    .foreach(item =&gt; infoAccumulator.add(item))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 运行结果: Set(3, 1, 2)</span></span><br><span class="line">  println(infoAccumulator.value)</span><br><span class="line"></span><br><span class="line">  sc.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="参考资料">参考资料</h3>
<p>https://knifefly.cn/2018/05/21/%E7%90%86%E8%A7%A3spark%E9%97%AD%E5%8C%85/（闭包）</p>
<p>https://blog.csdn.net/weixin_43854618/article/details/105680445（累加器）</p>
<p>https://www.cnblogs.com/qingyunzong/p/8890483.html#_label1（广播变量和累加器）</p>
]]></content>
      <categories>
        <category>Spark</category>
        <category>Spark Core</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark Core</tag>
      </tags>
  </entry>
  <entry>
    <title>Java集合操作</title>
    <url>/2021/04/11/Java%E9%9B%86%E5%90%88%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h3 id="java集合操作">Java集合操作</h3>
<p>遍历集合、Map类型数据排序</p>
<a id="more"></a>
<h3 id="遍历集合">遍历集合</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//Map.Entry遍历</span></span><br><span class="line"><span class="keyword">for</span> (Map.Entry&lt;String, Integer&gt; entry : tempMap.entrySet()) &#123;</span><br><span class="line">   String key = entry.getKey().toString();</span><br><span class="line">   String value = entry.getValue().toString();</span><br><span class="line">   System.out.println(<span class="string">&quot;key=&quot;</span> + key + <span class="string">&quot; value=&quot;</span> + value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//keySet()遍历</span></span><br><span class="line">  <span class="keyword">for</span> (Object o : tempMap.keySet()) &#123;</span><br><span class="line">   System.out.println(<span class="string">&quot;key=&quot;</span> + o + <span class="string">&quot; value=&quot;</span> + tempMap.get(o));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="map类型数据排序">Map类型数据排序</h3>
<p>注意TreeSet、TreeMap本身的Key就是按照字典序进行排序的。</p>
<h4 id="完整方法">完整方法</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">mapKeySort</span><span class="params">(HashMap&lt;Integer, String&gt; hm)</span> </span>&#123;</span><br><span class="line">     	<span class="comment">//定义一个list装载key、value</span></span><br><span class="line">        List&lt;Map.Entry&lt;Integer, String&gt;&gt; list = <span class="keyword">new</span> ArrayList&lt;Map.Entry&lt;Integer, String&gt;&gt;(hm.entrySet());</span><br><span class="line">        </span><br><span class="line">     	<span class="comment">//对list进行排序，自定义排序规则</span></span><br><span class="line">     	list.sort(<span class="keyword">new</span> Comparator&lt;Map.Entry&lt;Integer, String&gt;&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Map.Entry&lt;Integer, String&gt; o1, Map.Entry&lt;Integer, String&gt; o2)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> o1.getKey() &lt; o2.getKey() ? <span class="number">1</span> : ((o1.getKey() == o2.getKey()) ? <span class="number">0</span> : -<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">     </span><br><span class="line">     	<span class="comment">/*打印结果查看</span></span><br><span class="line"><span class="comment">        for (Map.Entry&lt;Integer, String&gt; mapping : list) &#123;</span></span><br><span class="line"><span class="comment">            System.out.println(mapping.getKey() + &quot;:&quot; + mapping.getValue());</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//最后把这个HashMap返回即可</span></span><br></pre></td></tr></table></figure>
<h4 id="简单方法">简单方法</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//获取key</span></span><br><span class="line">Object[] key_arr = hashmap.keySet().toArray();   </span><br><span class="line"><span class="comment">//对key排序</span></span><br><span class="line">Arrays.sort(key_arr);   </span><br><span class="line"><span class="comment">//通过key获取value</span></span><br><span class="line"><span class="keyword">for</span>(Object key : key_arr) &#123;   </span><br><span class="line">    Object value = hashmap.get(key);   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java基础</tag>
      </tags>
  </entry>
  <entry>
    <title>BasicVSR 论文阅读</title>
    <url>/2021/01/28/SR_Basicvsr/</url>
    <content><![CDATA[<p>《BasicVSR：The Search for Essential Components in Video Super-Resolution and Beyond》</p>
<h3 id="背景">背景</h3>
<p>相比图像超分任务，视频超分网络会设计更多模块，因为它多了一个时空维度。因此复杂的设计结构在视频超分网络中是常见的。本文重新分析了视频超分网络中的四大模块（对齐、聚合、传播和上采样）的作用，以及它们的优缺点。提出了一个视频超分框架baseline，即BasicVSR，并在Reds和Vimeo数据集上训练，验证了该框架的有效性。</p>
<p>另外，本文扩展BasicVSR框架，设计了信息重新填充机制和成对传播策略，促进信息聚合，即IconVSR网络。</p>
<a id="more"></a>
<h3 id="方法">方法</h3>
<p>该部分首先分析了视频超分网络中四大模块中不同选择的作用，并在每个模块中选择最好的组件构成BasicVSR网络，该网络可以作为设计其他视频超分网络的骨架backbone。然后，作者在backbone的基础上引入了两种新的组件信息重新填充机制和成对传播策略，略微提升了参数量和运行时间，并提高了网络性能。</p>
<h4 id="传播模块">传播模块</h4>
<p>本质上，传播模块定义了各个输入帧究竟是如何在网络中进行传播的，当前各个帧的传播方式主要有如下几种：</p>
<ul>
<li><p>Local</p>
<p>局部传播使用一个滑动窗口中的LR图像作为输入，然后利用这些局部信息完成重建任务。这是最常见的方式，也是TDAN、EDVR等方法的输入方式，例如一次性输入相邻的7帧，并选择重建第4帧，其他的作为参考帧。因此，网络能够访问的信息限制在这些局部相邻帧中。缺少远距离帧不可避免地限制了网络的表达能力。</p>
<p><img src="/2021/01/28/SR_Basicvsr/0.png" style="zoom:67%;"></p>
<p>如图所示，作者将测试集分成K个segments在BasicVSR网络中进行测试，可以看到当每个segments内的帧数越多的时候，其恢复的PSNR值越低，说明了帧的数量对网络有影响。</p>
<p>对于非recurrent形式的网络，滑动窗口内的帧数越多则会导致网络的参数量和占用显存都会大规模增加，这是一个主要的缺陷。</p></li>
<li><p>Unidirectional Propagation</p>
<p>单向传播形式，即如RLSP、RSDN等普通recurrent网络的形式，每次输入一帧和相邻帧，并利用重建的相邻帧及其中间产物来完成当前参考帧的重建过程。</p>
<p><img src="/2021/01/28/SR_Basicvsr/1.png" style="zoom:67%;"></p>
<p>然而，单向传播的方式会涉及到隐状态，即前一帧的中间产物的处理问题。本质上隐状态代表了前一帧或前n帧的特征，因此当输入的帧数越靠前时，隐状态含有的信息越少，对参考帧的重建效果越差，如上图所示。</p></li>
<li><p>Bidirectional Propagation</p>
<p>双向传播形式解决上述两种问题，本质上就是网络同时利用前一帧和后一帧的隐状态，来完成当前参考帧的重建。前一帧的隐状态包含当前参考帧之前的所有信息，后一帧的隐状态包含当前参考帧之后的所有信息，这相当于可以利用前后所有帧的信息。</p></li>
</ul>
<h4 id="对齐模块">对齐模块</h4>
<ul>
<li><p>无对齐</p>
<p>不对齐的特征或图像会影响聚合的效果，并最终导致性能降低。直接串联不对齐的特征以进行恢复，相邻帧的特征将不会与输入图像的特征在空间上对齐。由于卷积之类的局部操作具有相对较小的感受野，在相应位置集合信息时无法有效地利用相邻帧的信息，此时会导致较低的效率。</p></li>
<li><p>基于图像的对齐</p>
<p>基于图像的对齐使用光流估计相邻帧到参考帧的光流变化，然后利用光流的信息将相邻帧变形对齐到参考帧上。该过程是直接在图像上进行的，而非特征图上进行的。并且大部分的工作通常需要利用其他预训练的光流估计网络。</p></li>
<li><p>基于特征的对齐</p>
<p>基于特征的对齐最流行的方法是使用可变形卷积。然而本文使用的方式是基于光流的特征对齐，即先估计光流，然后根据光流将中间结果，即隐状态进行对齐，然后将对齐过后的特征和当前参考帧一起输入残差块进行重建。</p></li>
</ul>
<h4 id="聚合与上采样">聚合与上采样</h4>
<p>文章对聚合方式没有进行过多探讨，只采用最基础的聚合方式，即拼接特征图并输入多个卷积作为聚合模块。</p>
<p>上采样模块采用最经典有效的方式Pixel-shuffle。</p>
<h4 id="basicvsr">BasicVSR</h4>
<p><img src="/2021/01/28/SR_Basicvsr/2.png" style="zoom: 50%;"></p>
<p>本文提出的Backbone即BasicVSR，采用了上述探讨的最基础的几个模块。对齐模块采用基于光流的特征对齐，对齐的对象是隐状态和光流图。传播模块采用的是双向传播模式，当前参考帧需要利用前后一帧的隐状态。聚合模块是直接拼接，上采样模块则为Pixel-shuffle。</p>
<h4 id="iconvsr">IconVSR</h4>
<p>在BasicVSR的基础上引入了两种新的组件信息重新填充机制和成对传播策略来提升网络的性能。</p>
<ul>
<li><p>Information-Refill</p>
<p>信息重新填充机制是为了解决在图像边界和有遮挡区域的不精确对齐的问题。不精确对齐会导致误差的不断积加，特别是在网络中采用长距离传播的时候。为了解决特征不精确对齐带来的不利影响，本文提出了一个信息重新填充机制，以做特征修正。</p>
<p><img src="/2021/01/28/SR_Basicvsr/3.png" style="zoom:67%;"></p>
<p>如上图所示，E为特征提取模块，C为卷积单元。需要事先设定一些关键帧<span class="math inline">\(I_{key}\)</span>，当关键帧作为参考帧进行输入时，对齐的特征需要和前后帧的特征进行拼接，对当前的特征图进行修正，防止错误累加，最后才将对齐后的特征和参考帧一起输入重建模块。</p></li>
<li><p>Coupled Propagation</p>
<p>成对传播策略将后向传播的特征，即后向传播的隐状态，也作为正向传播模块的输入。而不是像BasicVSR中的那样，直接在U处融合前后两个传播分支的输出。</p>
<p><img src="/2021/01/28/SR_Basicvsr/4.png" style="zoom:67%;"></p>
<p>通过成对传播，前向传播分支从过去和将来的帧中接收信息，从而导致更高质量的特征，获得更好的输出。</p></li>
</ul>
<h3 id="实验结果">实验结果</h3>
<h4 id="与sota进行对比">与SOTA进行对比</h4>
<p><img src="/2021/01/28/SR_Basicvsr/5.png" style="zoom: 50%;"></p>
<p><img src="/2021/01/28/SR_Basicvsr/6.png" style="zoom: 50%;"></p>
<h4 id="信息重新填充机制和成对传播策略消融实验">信息重新填充机制和成对传播策略消融实验</h4>
<p><img src="/2021/01/28/SR_Basicvsr/7.png" style="zoom: 50%;"></p>
<h4 id="信息重新填充机制中的关键帧数量">信息重新填充机制中的关键帧数量</h4>
<p><img src="/2021/01/28/SR_Basicvsr/8.png" style="zoom: 50%;"></p>
<h3 id="总结">总结</h3>
<p>本文中提出的BasicVSR和IconVSR，利用recurrent的机制进行超分辨率。BasicVSR主要利用了双向传播机制，使得长距离信息能够得到利用，IconVSR在此基础上直接在前向传播分支中利用后向传播的特征，直接在特征维度上利用长距离信息。</p>
<p>同时注意到，双向传播机制的短板非常明显，无论是BasicVSR还是IconVSR都需要将所有视频帧全部输入到网络中之后才能够进行超分辨率，这是相比于滑动窗口最大的缺陷，这导致了基于双向传播的方法无法做到实时超分。并且，这需要保存大量的中间结果，即隐状态，这在训练时可能会导致大规模的显存占用情况存在。</p>
]]></content>
      <categories>
        <category>超分辨率</category>
        <category>VSR</category>
      </categories>
      <tags>
        <tag>VSR</tag>
      </tags>
  </entry>
  <entry>
    <title>DSFN 论文阅读</title>
    <url>/2021/02/09/SR_DSFN/</url>
    <content><![CDATA[<p>《Dual-Stream Fusion Network for Spatiotemporal Video Super-Resolution》 WACV 2021</p>
<h3 id="背景">背景</h3>
<p>时空视频超分辨率，即在时间维度上提升帧率，在空间维度上提升分辨率。本文认为，直接级联已有空间和时间超分辨率的方法（没有另外设计）来实现时空上采样，改变它们的顺序能够使得结果具有互补性。 因此，本文提出了一种双流融合网络，以自适应地融合两个时空上采样流产生的中间结果，其中第一个流应用空间超分辨率，然后是时间超分辨率，而第二个流则先使用空间超分辨率再进行时间超分辨率。</p>
<a id="more"></a>
<h3 id="方法">方法</h3>
<p><img src="/2021/02/09/SR_DSFN/0.png" style="zoom:50%;"></p>
<h4 id="框架的组件">框架的组件</h4>
<ul>
<li><p><span class="math inline">\(M_S\)</span></p>
<p>空间超分辨率网络。本文使用的是单幅图像的超分辨率网络ESPCN、SAN。</p></li>
<li><p><span class="math inline">\(M_T\)</span></p>
<p>时间超分辨率网络。即插帧网络，本文使用SuperSloMo、DAIN。</p></li>
<li><p><span class="math inline">\(F\)</span></p>
<p>融合网络，负责将不同分支产生的结果进行融合。具体采用采用了U-Net，它包含五个具有跳跃连接的对称下采样和上采样卷积层。</p></li>
<li><p><span class="math inline">\(R\)</span></p>
<p>调整网络，增强细节。具体为三个残差块。</p></li>
</ul>
<h4 id="框架的执行">框架的执行</h4>
<p>整个网络的流程具体来说就是将时空超分辨率分为两个分支，一个是先超分再插帧，另一个是先插帧再超分，然后通过融合网络融合两个分支的结果，最后通过调整网络进行细节调整。</p>
<p>空间超分辨率网络接收单个输入帧，并进行超分，输出对应一帧的超分结果；时间超分辨率网络接收前后两帧，并预测中间帧的结果。</p>
<p>先超分再插帧的过程可以表示为：</p>
<p><img src="/2021/02/09/SR_DSFN/1.png" style="zoom:67%;"></p>
<p>先插帧再超分的过程可以表示为：</p>
<p><img src="/2021/02/09/SR_DSFN/2.png" style="zoom:67%;"></p>
<p>本文认为，这两个数据流在时空上采样方面表现出互补的结果，其中先超分再插帧在运动较小的区域产生更精细的细节，而先插帧再超分在运动较大的区域提供更好的重建。</p>
<p>将双分支预测得到的结果进行融合，融合的策略就是给两个分支的结果分别预测一个Mask，并将Mask和预测结果进行element-wise的乘法后相加，整个过程可以表示为：</p>
<p><img src="/2021/02/09/SR_DSFN/3.png" style="zoom:67%;"></p>
<p>注意到，两个Mask之间，假如给予约束 Mask1 + Mask2 = 1，则总的预测结果变成了两个分支结果的线性插值，否则总的预测结果是两个分支结果的线性组合。</p>
<p>最终，通过一个三个残差块的调整网络增强细节信息。</p>
<p><img src="/2021/02/09/SR_DSFN/4.png" style="zoom:67%;"></p>
<h4 id="损失函数">损失函数</h4>
<p>损失函数主要分为两个部分：重建损失、辅助损失。</p>
<p><img src="/2021/02/09/SR_DSFN/5.png" style="zoom:77%;"></p>
<ul>
<li><p>重建损失</p>
<p>各个中间预测结果和GT的L1距离，作为重建损失。</p>
<p><img src="/2021/02/09/SR_DSFN/6.png" style="zoom:67%;"></p></li>
<li><p>辅助损失</p>
<p>主要是先超分再插帧的超分部分，以及先插帧再超分的插帧部分，对中间结果计算L1损失。</p>
<p><img src="/2021/02/09/SR_DSFN/7.png" style="zoom: 60%;"></p></li>
</ul>
<h3 id="实验结果">实验结果</h3>
<p>batch size设置为24，使用优化器RAdam，学习率开始时设置为<span class="math inline">\(5e-5\)</span>。时间和空间超分辨率均为2x。使用数据集Vimeo-90K、UCF101、FISR dataset。训练过程，首先单独训练超分网络和插帧网络，然后冻结参数把它们和融合网络及调整网络拼接在一起训练，最终联合训练所有模块。</p>
<h4 id="时空超分的互补结果">时空超分的互补结果</h4>
<p><img src="/2021/02/09/SR_DSFN/8.png"></p>
<p>文章认为先超分再插帧在小动作区域有更好的细节，先插帧再超分能够在大动作区域有更好的重建效果。如图所示应该是1、3行表示小动作区域，先超分再插帧的效果更好，Error Map有更少的像素点有值；而2、4行则表示大动作区域，先插帧再超分的Error Map有更少的像素点有值。</p>
<h4 id="不同融合策略的比较">不同融合策略的比较</h4>
<p><img src="/2021/02/09/SR_DSFN/9.png" style="zoom:67%;"></p>
<p>One-mask表示Mask1 + Mask2 = 1，总的预测结果变成了两个分支结果的线性插值，Two-mask则表示总的预测结果是两个分支结果的线性组合，可以看出Two-mask更有优势。</p>
<h4 id="选择不同上采样网络">选择不同上采样网络</h4>
<p><img src="/2021/02/09/SR_DSFN/10.png" style="zoom: 67%;"></p>
<p>越新的方法作为backbone效果则越好。</p>
<h4 id="与sota结果相比">与SOTA结果相比</h4>
<p><img src="/2021/02/09/SR_DSFN/11.png" style="zoom:67%;"></p>
<p>并没有和最新的网络结果相比，例如Zooming Slow-Mo。</p>
<h3 id="结论">结论</h3>
<p>本文提出了一种融合双分支结果的网络来进行时空超分辨率。然而，该网络并不是end-to-end训练的，借用已有的网络进行多阶段训练，并且正文没有给出参数量和计算量，可能代价也比较高。其次，重建完全由中间和最终输出帧的像素级重建损失来指导，把超分和插帧的过程都看成是独立的过程，各个中间结果之间没有加上时序的联系，这必然导致了性能的降低。因此，在时空超分中如何有效地利用时序信息进行更好的重建值得进行探索。</p>
]]></content>
      <categories>
        <category>超分辨率</category>
        <category>STVSR</category>
      </categories>
      <tags>
        <tag>STVSR</tag>
      </tags>
  </entry>
  <entry>
    <title>DualSR 论文阅读</title>
    <url>/2021/02/05/SR_DualSR/</url>
    <content><![CDATA[<p>《DualSR: Zero-Shot Dual Learning for Real-World Super-Resolution》 WACV 2021</p>
<h3 id="背景">背景</h3>
<p>许多基于深度学习的SR方法在庞大数据集上学习复杂的LR-HR上采样关系。但是，这些经过预训练的SR方法通常在直接从相机捕获的图像上表现差很多。它们接受了干净，无噪声，合成的LR图像进行训练，而真实LR图像的退化过程与理想条件不同。现实情况下，每个摄像机的采集参数，例如传感器的点扩展功能（PSF），也不同。即使是同一台摄像机拍摄的图像，也会因光线条件、景深以及抖动而产生的模糊等而有所不同。这些条件使得训练一个在所有不同图像退化条件下都能表现良好的CNN变得很困难。</p>
<a id="more"></a>
<p>许多盲SR方法在超分之前都会估算退化过程。 盲超分的SOTA使用深度学习来学习图像特定的下采样器（降级模型参数），上采样器使用该下采样器对输入的LR图像进行超分辨。 但是，从单个输入图像估计合适的下采样器很复杂。 尤其是在存在噪声或其他采集伪像的情况下，这些方法通常无法估算出良好的降级参数。 错误的降级会严重降低上采样器的效率，并降低SR性能。</p>
<p>受诸如CycleGAN 和 DualGAN 等最新无监督方法的启发，论文引入了Zero-Shot DualSR，一种双路径架构，用于在现实世界中的LR图像上实现超分辨率，并且是一种无监督的方案。</p>
<h3 id="方法">方法</h3>
<p><img src="/2021/02/05/SR_DualSR/0.png" style="zoom: 50%;"></p>
<h4 id="框架的组件">框架的组件</h4>
<ul>
<li><p><span class="math inline">\(G_{UP}\)</span></p>
<p>上采样器。与ZSSR类似，采用了简单的8层全卷积网络，并采用了ReLU激活。在输入和输出之间存在一个全局的残差连接。将LR图像放大到输出大小，然后才将其送入网络。</p></li>
<li><p><span class="math inline">\(G_{DN}\)</span> &amp; <span class="math inline">\(D_{DN}\)</span></p>
<p>下采样器和判别器。使用KernelGAN中的生成器和判别器。</p>
<p>生成器是一个深层线性网络（没有任何激活）。小的感受野强迫使网络仅使用LR图像的局部特征（例如边缘），而不是依赖于高级全局特征。 因此，生成器<span class="math inline">\(G_{DN}\)</span>学习能够生成图像的kernel，该图像在patch分布上与输入LR图像相似。</p>
<p><img src="/2021/02/05/SR_DualSR/1.png" style="zoom:55%;"></p>
<p>判别器是一个全卷积的PatchGAN，其感受野为7x7。</p>
<p><img src="/2021/02/05/SR_DualSR/2.png" style="zoom:60%;"></p></li>
</ul>
<h4 id="框架的执行">框架的执行</h4>
<p>在前向循环中，首先应用上采样器以生成2x的上采样图像。，然后应用下采样器并将上采样的图像转换回1x。</p>
<p>同样，在后向循环中，首先由<span class="math inline">\(G_{DN}\)</span>生成1/2x的图像，然后<span class="math inline">\(G_{UP}\)</span>将图像上采样回到原始比例。</p>
<h4 id="损失函数">损失函数</h4>
<p>损失函数主要分为三部分：对抗损失、循环一致性损失、掩码插值损失。</p>
<p><img src="/2021/02/05/SR_DualSR/3.png" style="zoom: 67%;"></p>
<ul>
<li><p>对抗损失</p>
<p>对于下采样器<span class="math inline">\(G_{DN}\)</span>，希望其能够生成和真实LR相同的图像，判别器尽可能认为它是真实的LR（标签为1），损失如下（正则化项<span class="math inline">\(R\)</span>具体见KernelGAN论文）：</p>
<p><img src="/2021/02/05/SR_DualSR/4.png" style="zoom:60%;"></p>
<p>对于判别器<span class="math inline">\(D_{DN}\)</span>,希望其能够判别哪些是真实的LR，即让真实的LR被判别为1，合成的LR被判别为0：</p>
<p><img src="/2021/02/05/SR_DualSR/5.png" style="zoom:60%;"></p></li>
<li><p>循环一致性损失</p>
<p>确保<span class="math inline">\(G_{UP}\)</span>和<span class="math inline">\(G_{DN}\)</span>可以还原由另一个执行的操作。</p>
<p><img src="/2021/02/05/SR_DualSR/6.png" style="zoom:60%;"></p></li>
<li><p>掩码插值损失</p>
<p>Bicubic上采样能够能够正确地对低频部分进行上采样，但是无法重构高频细节。对所有像素使用Bicubic能够产生无伪影但模糊的结果。因此，论文仅对图像低频部分应用插值损失。首先对Bicubic上采样的图像应用Sobel算子，它主要用作边缘检测，是一种离散性差分算子，用来计算图像亮度函数的灰度近似值。应用Sobel算子能够产生mask，该mask在低频区域的像素值较高，在图像的高频区域的像素值较低。</p>
<p><img src="/2021/02/05/SR_DualSR/7.png" style="zoom:60%;"></p>
<p>然后应用掩码插值损失使得<span class="math inline">\(G_{UP}\)</span>的结果和Bicubic上采样的结果仅在低频部分是相近的。</p>
<p><img src="/2021/02/05/SR_DualSR/8.png" style="zoom:60%;"></p></li>
</ul>
<h3 id="实验结果">实验结果</h3>
<p>训练使用的 patch size 为 64 x 64 以及 128 x 128。训练和测试时不使用任何数据增强（图像变换）。因为训练时间很少，所以可以用暴力搜索方法来获得<span class="math inline">\(\lambda_{cycle}=5,\lambda_{inter}=2\)</span>。</p>
<p>在RTX 2080 Ti GPU上，论文的方法平均训练+推理时间为233秒。 对于KernelGAN + ZSSR 的组合，运行时间为281秒，对于BlindSR，则为370秒。 像SAN这样的有监督深度学习SR方法具有很长的训练时间，并且图像大小显着影响其推理时间。对于SAN +，在DIV2KRK基准上测试每张图片平均需要298秒。</p>
<h4 id="合成的真实lr数据集">合成的真实LR数据集</h4>
<p><img src="/2021/02/05/SR_DualSR/9.png" style="zoom: 45%;"></p>
<h4 id="realsr数据集">RealSR数据集</h4>
<p>该数据集通过调整焦距获得不同尺度图像，但作者认为不同尺度间没有完全对齐，因此只放出视觉效果。</p>
<p><img src="/2021/02/05/SR_DualSR/10.png" style="zoom:40%;"></p>
<h4 id="掩码插值损失">掩码插值损失</h4>
<p><img src="/2021/02/05/SR_DualSR/11.png" style="zoom:50%;"></p>
<h3 id="结论">结论</h3>
<p>本文提出提出了DualSR，一个轻量级的dual架构，它学习每个图像特定的LR-HR关系。它由下采样器和上采样器组成，在训练中使用循环一致性损失来相互改进。此外，论文提出了掩码插值损失，消除了图像低频区域的伪影，而不会导致边缘过于平滑。</p>
<p>该方法是Zero-Shot的，无需HR图像进行监督，但本质上就是用了Bicubic上采样的低频部分来进行监督。之前的文章DynaVSR是用了元学习的思想，在测试时同样使用了类似循环一致性的方法，但在训练时为了更好的效果仍然使用了HR的图像。因此，可以思考如何将Zero-Shot的思想（目前来看还是想办法在Bicubic上采样的结果做文章），应用到VSR上，来达到完全无监督的效果。</p>
]]></content>
      <categories>
        <category>超分辨率</category>
        <category>SISR</category>
      </categories>
      <tags>
        <tag>SISR</tag>
        <tag>RealSR</tag>
      </tags>
  </entry>
  <entry>
    <title>TPSTVSR 论文阅读</title>
    <url>/2021/02/26/SR_TPSTVSR/</url>
    <content><![CDATA[<p>《Space-Time Video Super-Resolution Using Temporal Profiles》 ACMMM 2020</p>
<h3 id="背景">背景</h3>
<p>在时空超分辨率中，直接将VFI（插帧）和VSR（超分）串联，不能充分利用视频的时空相关性，是一种次优做法。此外，在计算效率低的同时，也容易引入累积误差。Zooming Slow-Mo提出一种one-stage的做法，使用可变形的ConvLSTM来隐式对齐帧，这样可能会错过长依赖的时间上下文，因为当更多的帧融合时，需要设计更复杂的帧对齐规则。因此，本文提出利用 Temporal Profile (TP)来解决时空超分辨率问题。</p>
<a id="more"></a>
<p>视频帧在一个Patch上，可以转换为如下水平（vertical）TPs 和垂直（horizontal）TPs：</p>
<p><img src="/2021/02/26/SR_TPSTVSR/0.png" style="zoom: 50%;"></p>
<p>基于TP的时空超分辨率有以下几个好处：</p>
<p>（1）STVSR可以有效地建模为基于学习的恢复任务，聚焦于TPs的特定二维结构。</p>
<p>（2）TPs既包含空间维度又包含时间维度，可以更好地利用时空相关性。</p>
<p>（3）与现有的多帧对齐方法相比，TPs能够更灵活地融合长依赖的时间上下文信息。</p>
<h3 id="方法">方法</h3>
<p>首先将低帧率和低分辨率的视频转换成TPs，然后将其送入Temporal Profile超分辨率模块（TPSRM）提高帧率，然后将TPs转换为视频域，发送给特征洗牌模块（FSM）生成具有目标空间分辨率的视频，最后通过调整模块（RM）去除伪影，增强细节。</p>
<p><img src="/2021/02/26/SR_TPSTVSR/1.png" style="zoom: 80%;"></p>
<ul>
<li><p>Temporal-Profile Super-Resolution Module</p>
<p>首先将输入视频帧 W x H x T 转换为垂直Temporal Profile T x H x W（转换为水平TP无太大区别），即在垂直方向上将视频帧切割成W个。然后输入TPSRM，将TP分辨率提高为 （2T-1）x 2H x W。TPSRM采用IMDN模型，用L1损失函数做监督，ground truth由高帧率高分辨率视频转换成TP得到。</p>
<p><img src="/2021/02/26/SR_TPSTVSR/8.png" style="zoom:60%;"></p></li>
<li><p>Feature Shuffling Module</p>
<p>该模块的目的是提高空间分辨率。经过TPSRM后，帧率提升为原来的两倍，H提高为原来的两倍，而W不变，FSM的目的就是将H再次提高两倍，W直接提高四倍。注意到其中Feature Shuffling阶段会将通道数（也就是帧率）减半，来将W提升2倍，在后面的模块需要将空间分辨率和通道数同时提升。CSR同样使用IMDN模块。</p>
<p><img src="/2021/02/26/SR_TPSTVSR/2.png" style="zoom: 67%;"></p></li>
<li><p>Refining Module</p>
<p>该模块目的是消除伪影和增强细节，采用U-Net结构。</p>
<p><img src="/2021/02/26/SR_TPSTVSR/3.png" style="zoom: 67%;"></p></li>
</ul>
<p>损失函数采用 L1、SSIM、VGG和Cycle Consistency四种损失函数。采用循环一致性损失来保证重构视频与输入的时空一致性，同时可以避免过度增强。</p>
<p><img src="/2021/02/26/SR_TPSTVSR/4.png" style="zoom:67%;"></p>
<p><img src="/2021/02/26/SR_TPSTVSR/5.png" style="zoom:67%;"></p>
<p><img src="/2021/02/26/SR_TPSTVSR/6.png" style="zoom:67%;"></p>
<p><img src="/2021/02/26/SR_TPSTVSR/7.png" style="zoom: 67%;"></p>
<h3 id="实验结果">实验结果</h3>
<p>使用Vimeo90K数据集进行训练，进行空间4倍超分，时间2倍超分，batch size设置为1，学习率每2个epoch下降0.2，在一张1080Ti GPU上进行训练。</p>
<h4 id="与sota对比">与SOTA对比</h4>
<p><img src="/2021/02/26/SR_TPSTVSR/9.png" style="zoom:80%;"></p>
<h4 id="参数量与运行时间">参数量与运行时间</h4>
<p><img src="/2021/02/26/SR_TPSTVSR/10.png" style="zoom:80%;"></p>
<h4 id="不同损失函数比较">不同损失函数比较</h4>
<p><img src="/2021/02/26/SR_TPSTVSR/11.png" style="zoom:67%;"></p>
<h4 id="真实场景老电影恢复">真实场景：老电影恢复</h4>
<p>由于相机设备的分辨率有限，老电影往往会出现严重的时空退化。另外，在不同的压缩程度下保存也会影响观感。因此时空超分有应用场景。从网络上直接下载老电影，并和Zooming Slow-Mo进行对比，两者同样在Vimeo90K上训练。</p>
<p><img src="/2021/02/26/SR_TPSTVSR/12.png" style="zoom: 67%;"></p>
<h3 id="结论">结论</h3>
<p>本文从Temporal Profile的角度进行时空超分辨率，与Zooming Slow-Mo相比降低了参数量和运行时间。该网络利用的是滑动窗口的原理，当帧数过多时会造成显存大量占用的问题。此外，当一个移动的物体在视频中突然出现或消失时，由于快速的移动，TPs很难捕捉到全局信息，融合水平和垂直的TPs可能是一种解决方案。</p>
<p><img src="/2021/02/26/SR_TPSTVSR/13.png" style="zoom:67%;"></p>
]]></content>
      <categories>
        <category>超分辨率</category>
        <category>STVSR</category>
      </categories>
      <tags>
        <tag>STVSR</tag>
      </tags>
  </entry>
  <entry>
    <title>TriNAS 论文阅读</title>
    <url>/2021/01/29/SR_TriNAS/</url>
    <content><![CDATA[<p>《Trilevel Neural Architecture Search for Efficient Single Image Super-Resolution》</p>
<h3 id="背景">背景</h3>
<p>传统的基于深度学习的超分辨率方法通常会在三层神经结构设计上选择以下变化：网络级优化、单元（cell）级优化以及内核（kernel）级优化。为一个深层SR模型人工执行这些优化需要较高的代价。并且，人工设计的架构往往不是最优的，对于真实超分辨率来说，可能在计算上效率低下。因此，论文提出了一种用于高效单图像超分辨率（SR）的三级神经架构搜索（TriNAS）方法。</p>
<a id="more"></a>
<p>论文首先在三个级别（即网络级别，单元级别和内核级别（卷积内核））定义离散搜索空间。提出了基于树模型的搜索框架来替代网格型的搜索框架，减少节点之间的依赖性。</p>
<p>然后，与之前利用softmax进行连续松弛策略的NAS方法不同，论文利用排序的sparsestmax来使得三级搜索架构稀疏地起作用。因此，论文的NAS优化可以逐渐收敛到对超网起主要作用的神经架构。</p>
<p>此外，论文提出的方法可以在单个阶段中同时进行搜索和训练，与传统的NAS算法相比，这大大减少了搜索和训练时间。</p>
<p>在基准数据集上进行的实验表明，论文的的NAS算法所提供的SR模型在参数数量和FLOPS方面具有显着减轻的优势，其PSNR值可与当前的SOTA相媲美。</p>
<h3 id="方法">方法</h3>
<h4 id="搜索空间定义">搜索空间定义</h4>
<p><img src="/2021/01/29/SR_TriNAS/2.png" style="zoom: 60%;"></p>
<p>为了在模型容量和模型大小之间达到适当的平衡，同时保证最小的精度损失，论文提出在内核级、单元级和网络级结构上进行搜索，以得到高效的SR网络结构。</p>
<ul>
<li><p>网络级</p>
<p>论文遵循AGD和SRResNet来定义网络级搜索空间。他们的超网结构使大多数计算都在低分辨率特征空间中进行，从而提高了计算效率。</p>
<p>网络级架构搜索主要通过搜索五个残差块和两个上采样块。将Residual-in-Residual（RiR）模块中的dense block替换为包含可搜索单元级运算符和内核级运算的五个连续层。</p>
<p>为了进行高效的上采样模块设计，论文替换了两个最初设计的上采样模块。取代的是一个PixelShuffel块，其中包含一个卷积层以及一个PixelShuffel层，以达到上采样的目的。由于PixelShuffle块固定在网络的尾部，因此在网络级搜索空间中添加了两个常规卷积层。最终，论文仅关注堆叠的五个RiR块和两个用于网络路径搜索的标准卷积层。</p>
<p><img src="/2021/01/29/SR_TriNAS/0.png" style="zoom:80%;"></p>
<p>为了对网络级搜索空间建模，采用上图所示的AutoDeep-Lab网格状结构似乎是可行的解决方案。 但是，网格建模旨在遍历网络块的所有顺序路径。 每个节点表示这个位置的feature map，每个路径都从第一个节点开始，并沿着一组箭头到达最终的目标节点。 显然，所有路径共享大多数节点和箭头。 这种冗余共享导致路径，单元和内核之间的极端依赖，因为路径在层次上包括它们。 尽管这样的共享策略可以节省大量训练内存，但它极大地限制了搜索空间。 此外，紧密的结合可能会损害每条路径的贡献以及对某些冗余路径的修剪的学习。</p>
<p><img src="/2021/01/29/SR_TriNAS/1.png" style="zoom: 67%;"></p>
<p>论文提出了一种用于网络级路径搜索建模的树结构，来克服冗余共享的缺点。 如上图所示，树建模旨在遍历所有树结构路径。 在这里，每个节点仅连接到其父节点和子节点，因此，依赖性非常宽松。 但是，必须在训练时维护这样的关联，以降低内存消耗。 放宽不同路径的相关性可以实现灵活的网络级搜索空间。 对路径的较低依赖性可能会由于单元和内核之间的分层连接而导致它们之间可靠的关联，从而使它们的搜索空间更为通用。 此外，引入的树模型可以更好地解开路径之间的纠缠关系，从而能够对冗余路径进行修剪。</p></li>
<li><p>单元级</p>
<p>在单元级，论文搜索五个RiR块，每个块包含五个个可搜索的单元，即总共25个可搜索的单元。每个单元会选择如下操作符：</p>
<p>Conv 1×1；Residual Block (2 layers of Conv 3×3 + skip-connection)；Conv 3×3；Depthwise Block (Conv 1×1 + Depthwise Conv3×3 + Conv 1×1).</p></li>
<li><p>内核级</p>
<p>对于内核级搜索，论文遵循super-kernel框架对该搜索空间进行建模。 对于每个卷积内核，先设置一个具有完整通道的超级内核。 为了修剪超级内核的通道数，需要一组可搜索的扩展比<span class="math inline">\(\phi=[\frac{1}{3},\frac{1}{2},\frac{4}{5},\frac{5}{6},1]\)</span>。并且设置参数<span class="math inline">\(\gamma_i\)</span>控制选择第<span class="math inline">\(i\)</span>个扩展比的概率。</p></li>
</ul>
<h4 id="连续松弛策略">连续松弛策略</h4>
<p>为了使得NAS是可微的，关键思想是将离散的搜索空间的显式选择放宽为搜索空间中所有相关候选对象的的隐式选择。连续松弛使我们能够以完全可微的方式选择对超网贡献最大的候选者，然后就可以通过反向传播的方式优化整个超网，以实现高效的架构。流行的连续松弛策略之一是应用softmax来实现所有候选网络操作的混合。但是，softmax无法产生稀疏的分布，因此，它无法反映主导操作，这对于有效的结构设计而言至关重要。因此，使用softmax可以防止超级网络收敛到主要的候选架构。</p>
<p>为了解决这个问题，论文提出了sparsestmax，它在连续松弛过程中产生了良好的稀疏性，并且可以寻求具有优势的候选架构，同时具有诸如softmax之类的凸性和可微性。</p>
<p>具体来说，为了实现离散网络级搜索空间的连续松弛，论文使用一组连续组合权值来聚合所有的网络路径，构成一个超网络。网络级架构搜索的树模型中每个特征图（即节点）都可以用作其对应路径的输出，然后可以将输出送到上采样层以获得超分结果。 论文根据提出的树模型对来自相关网络路径的所有特征图定义了一组权重<span class="math inline">\(\beta\)</span>，因此，超网的输出是所有中间特征图的加权组合。</p>
<p>传统的连续松弛做法就是对所有的<span class="math inline">\(\beta\)</span>应用softmax函数来判断每个路径对总的网络的贡献。然而，softmax通常产生非零参数，即平滑变化的参数。因此，候选路径的贡献是相对均匀的，这防止超网络收敛到一个主要的候选结构。因此，利用sparsetmax来产生稀疏的分布，其中<span class="math inline">\(q\)</span>表示具有约束的单纯形：</p>
<p><img src="/2021/01/29/SR_TriNAS/3.png" style="zoom: 80%;"></p>
<p>sparsestmax的基本思想是将输入向量<span class="math inline">\(\beta\)</span>的欧几里德投影投射到概率单纯形上。该投射可能会触及单纯形边界，在这种情况下，sparsestmax会产生稀疏分布。为了获得更好的稀疏度，sparsestmax还引入了一个圆环约束，该圆环约束可以通过将单纯形外接圆的半径从零线性增加到某个阈值来逐步产生稀疏性。</p>
<p>前文提到的树模型搜索架构有助于对路径进行剪枝，然而直接使用sparsestmax可能会在节点上产生无序的非零组合权重（极端情况是它们全部分布在奇/偶数节点上）。 在这种情况下，除非排序稀疏度，否则无法很好地修剪路径。 换句话说，只要非零组合权重沿路径下降，以便所有零权重都出现在路径的尾部，就可以执行网络级修剪，直接删除尾部即可。因此，论文提出利用排序的sparsetmax方法。 对每个路径<span class="math inline">\(p_i\)</span>内的权重<span class="math inline">\(\beta_i\)</span>施加排序约束。 直观上，这有助于使得浅层的输出feature maps对超网趋于相同的贡献度。排序的sparsetmax可以表示为：</p>
<p><img src="/2021/01/29/SR_TriNAS/4.png" style="zoom: 77%;"></p>
<p>上述讨论了网络级的连续松弛策略，即使用了排序的sparsetmax方法。对于单元级的连续松弛策略，同样给每个单元的输出定义权重<span class="math inline">\(\alpha\)</span>，并采用非排序的sparsetmax的方式进行连续松弛。对于内核级的连续松弛策略，则采用gumbel-softmax方法。</p>
<h4 id="代理任务和优化">代理任务和优化</h4>
<p>对于本文的三级NAS任务，论文不是从头训练模型，而是通过知识蒸馏方法，从而利用预先训练的最先进的图像超分辨率模型的知识。将预训练的ESRGAN模型作为teacher model，搜索阶段的代理任务是通过最小化模型输出与教师模型输出之间的知识精馏距离来搜索模型G。此外，图像SR任务偏向于更有效率的模型，因此在目标函数中加入模型效率项。</p>
<p>由于架构搜索的参数的数量远远小于网络模型的参数的数量，因此在单个训练集上对它们进行联合优化容易出现过拟合。具体来说，将数据集分为训练集和验证集，分别在这两组数据上优化网络参数和架构搜索参数。</p>
<h4 id="算法流程">算法流程</h4>
<p><img src="/2021/01/29/SR_TriNAS/7.png"></p>
<h3 id="实验结果">实验结果</h3>
<h4 id="与sota进行对比">与SOTA进行对比</h4>
<p><img src="/2021/01/29/SR_TriNAS/5.png" style="zoom: 50%;"></p>
<h4 id="softmax与sparsetmax对比">softmax与sparsetmax对比</h4>
<p><img src="/2021/01/29/SR_TriNAS/6.png" style="zoom: 67%;"></p>
<h3 id="结论">结论</h3>
<p>本文介绍了用于单图像超分辨率任务的Trilevel NAS方法，主要是将排序的sparsetmax激活用于树模型的网络架构搜索中。树模型能够使得节点之间降低依赖性，提供灵活的搜索空间，并能够更好地进行剪枝优化；sparsetmax激活在连续松弛过程中产生了良好的稀疏性，使得超网能够收敛到一个主要的候选结构上；排序的sparsetmax同样能够更好地进行模型剪枝。</p>
<p>论文中提到的sparsetmax仅使用在了网络级和单元级架构搜索，可以同样利用排序的sparsetmax去进一步地优化内核级的搜索空间，来产生更高效的超分模型，以达到高PSNR和高感知质量的效果。</p>
]]></content>
      <categories>
        <category>超分辨率</category>
        <category>SISR</category>
      </categories>
      <tags>
        <tag>SISR</tag>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title>Java位运算</title>
    <url>/2021/04/12/Java%E4%BD%8D%E8%BF%90%E7%AE%97/</url>
    <content><![CDATA[<h3 id="位运算">位运算</h3>
<p>计算机里面任何数据本质上是用二进制（0 1）来保存的数字。位运算就是直接对二进制类型的数据进行计算。</p>
<a id="more"></a>
<table>
<thead>
<tr class="header">
<th>符号</th>
<th>描述</th>
<th>运算规则</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>&amp;</td>
<td>与</td>
<td>两位都为1，结果为1</td>
</tr>
<tr class="even">
<td>|</td>
<td>或</td>
<td>有一位为1，结果为1</td>
</tr>
<tr class="odd">
<td>~</td>
<td>非</td>
<td>~0 = 1， ~1 = 0</td>
</tr>
<tr class="even">
<td>^</td>
<td>异或</td>
<td>两位不相同，结果为1</td>
</tr>
<tr class="odd">
<td>&lt;&lt;</td>
<td>左移</td>
<td>二进制位全部左移，高位丢弃，低位补0</td>
</tr>
<tr class="even">
<td>&gt;&gt;</td>
<td>右移</td>
<td>二进制位全部右移，无符号数高位补零，有符号数补符号位或零（根据编译器）</td>
</tr>
</tbody>
</table>
<h3 id="常用位操作">常用位操作</h3>
<ul>
<li><p>判断奇偶</p>
<p>(x &amp; 1) == 1 等价 (x % 2 == 1)</p>
<p>(x &amp; 1) == 0 等价 (x % 2 == 0)</p></li>
<li><p>除以二</p>
<p>x &gt;&gt; 1 等价 x / 2</p></li>
<li><p>把最低位的二进制1去掉</p>
<p>x &amp;= (x - 1)</p></li>
<li><p>得到最低位的1</p>
<p>x &amp; -x</p>
<blockquote>
<p>-x的运算是，所有位置取反，然后+1</p>
<p>保留二进制下最后出现的1的位置，其余位置置0</p>
<p>https://www.cnblogs.com/yzxag/p/12668034.html</p>
</blockquote></li>
<li><p>得到0</p>
<p>x &amp; ~x == 0</p></li>
</ul>
<h3 id="指定位置的位运算">指定位置的位运算</h3>
<p>https://leetcode-cn.com/problems/power-of-two/solution/5chong-jie-fa-ni-ying-gai-bei-xia-de-wei-6x9m/</p>
]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java基础</tag>
        <tag>位运算</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper</title>
    <url>/2021/04/12/%E5%A4%A7%E6%95%B0%E6%8D%AEZookeeper/</url>
    <content><![CDATA[<p>Zookeeper（分布式协调服务）帮助Hadoop解决的问题</p>
<a id="more"></a>
<p>主要是为了实现高可用机制</p>
<p>首先所有的NN在启动的时候会竞争写一个zookeeper上的临时节点，所有的standby nn向这个节点注册一个观察器，当这个节点出现异常或挂掉时，起在zookeeper上创建的临时节点也会被删除，standy的nn节点检测到该节点发生变化时，会重新发起竞争，直到产生一个Active节点。</p>
<p>写入高可用。 集群中的写入操作都是先通知Leader，Leader再通知Follower写入，实际上当超过一半的机器写入成功后，就认为写入成功了，所以就算有些机器宕机，写入也是成功的。</p>
<p>读取高可用。 zookeeperk客户端读取数据时，可以读取集群中的任何一个机器。所以部分机器的宕机并不影响读取。 zookeeper服务器必须是奇数台，因为zookeeper有选举制度，角色有：领导者、跟随者、观察者，选举的目的是保证集群中数据的一致性。</p>
<p>https://blog.csdn.net/eric_sunah/article/details/46610167</p>
<p>https://www.jianshu.com/p/87976ec5f45f</p>
<p>kafka</p>
<p>分布式的发布-订阅消息系统。</p>
<p>Kafka可以将主题划分为多个分区（Partition），会根据分区规则选择把消息存储到哪个分区中，如果分区规则设置的合理，那么所有的消息将会被均匀的分布到不同的分区中，这样就实现了负载均衡 和水平扩展。</p>
<p>一个分区可以有多个副本，这些副本保存在不同的broker上。每个分区的副本中都会有一个作为 Leader。当一个broker失败时，Leader在这台broker上的分区都会变得不可用，kafka会自动移除 Leader，再其他副本中选一个作为新的Leader。</p>
<p>消息同步：先写入到leader中，follower再同步。Producer只将该消息发送到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致。Follower在收到该消息并写入其Log后，向Leader发送ACK。一旦Leader收到了ISR中的所有Replica的ACK，该消息就被认为已经commit了，Leader将增加HW（即offset）并且向Producer发送ACK。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>OR-Net 论文阅读</title>
    <url>/2021/02/16/SR_ORNet/</url>
    <content><![CDATA[<p>《Learning Omni-frequency Region-adaptive Representations for Real Image Super-Resolution》 2021</p>
<h3 id="背景">背景</h3>
<p>Real World数据包含不同的、复杂的退化方式，例如模糊、噪声和降采样，因此导致传统深度学习方法在真实数据集上表现不好。</p>
<p>RealSR和DRealSR不仅提出了两个真实数据集，还提出了两种真实超分方法 laplacian pyramid based kernel prediction network (LP-KPN) 和 component divide-and- conquer (CDC)。然而，它们都是在像素水平上进行重建（区域、边缘和角落），没有利用到频率信息。</p>
<a id="more"></a>
<p>图像的频率是表征图像中灰度变化剧烈程度的指标，是灰度在平面空间上的梯度。图像的主要成分是低频信息，它形成了图像的基本灰度等级，对图像结构的决定作用较小；中频信息决定了图像的基本结构，形成了图像的主要边缘结构；高频信息形成了图像的边缘和细节，是在中频信息上对图像内容的进一步强化。</p>
<p><img src="/2021/02/16/SR_ORNet/0.png" style="zoom: 50%;"></p>
<p>作者观察到，对于图a可以看出，LR的退化在各个频率上都存在。对于图b可以看出，在不同区域中的不同频率组件内，真实LR的退化也是不同的。</p>
<p>因此，本文作者提出ORNet，在图像的不同频率上增强相应的频率因子，最终通过区域自适应频率聚合模块，结合动态卷积和空间注意力机制，针对HR图像的不同位置，有选择性地重建不同的频率分量。</p>
<h3 id="方法">方法</h3>
<p><img src="/2021/02/16/SR_ORNet/1.png" style="zoom: 67%;"></p>
<h4 id="框架的组件">框架的组件</h4>
<ul>
<li><p>Frequency Decomposition (FD) Module</p>
<p>对图像进行频率分离，可以在传统的信号处理方法中采用小波变换或离散余弦变换来实现。然而，由于数学运算的确定性和任务无关性，这种转换不可避免地为low-level恢复任务丢弃了一些关键、详细的信息。为了在模拟小波变换的同时避免关键信息丢失，作者提出通过可学习的隐空间下采样，来分解混合特征表示。类似的操作可以参考OctConv。</p>
<p>具体来说，作者首先利用步长较大的卷积层(如stride= 2)对特征表示进行下采样，提取粗特征，即低频分量。然后，从原始特征(下采样前)中减去这些相对低频的分量，以获得其余相对高频的特征表示。</p></li>
<li><p>Frequency Enhancement Unit (FEU)</p>
<p>该单元对不同频率的信息进行增强。如b种所示结构，将CA的结果和普通卷积的结果相拼接，并加入dense connection。目的是有助于各分支在不同阶段选择性融合相应的频率分量，从而提高各分支在不同频域的表示能力。</p></li>
<li><p>Region-adaptive Frequency Aggregation (RFA) Module</p>
<p>该模块对不同频率组件进行自适应聚合。</p>
<p><img src="/2021/02/16/SR_ORNet/4.png" style="zoom:50%;"></p>
<p>即预测多个动态卷积核，并设置一个可学习的权重向量，将多个卷积核组合在一起形成动态卷积核，通过它生成attention map并与原来的特征图做乘法。</p></li>
</ul>
<h4 id="框架的执行">框架的执行</h4>
<p>首先将输入图像进行频率分离。用大步长卷积提取低频特征，并不断减去低频特征获得高频特征。</p>
<p><img src="/2021/02/16/SR_ORNet/2.png" style="zoom: 67%;"></p>
<p>上箭头表示双线性插值上采样，下箭头表示带步长卷积进行下采样。</p>
<p>接着，通过FEU来增强不同频率的表示，以弥补低/中/高频信息的丢失，并将低频信息与不同高频信息相结合。</p>
<p><img src="/2021/02/16/SR_ORNet/3.png" style="zoom: 60%;"></p>
<p>真实LR图像中不同区域的频率信息丢失是不同的。因此，有必要对不同区域的全频分量进行自适应聚合，以恢复更真实、纹理细节更丰富的HR图像。通过作者提出的RFA模块，使用动态卷积核的方式生成attention map来组合多个频率下的组件，最终通过pixel shuffle提高分辨率。</p>
<p><img src="/2021/02/16/SR_ORNet/5.png" style="zoom:60%;"></p>
<h3 id="实验结果">实验结果</h3>
<p>是一种有监督方法，使用DRealSR进行训练，动态核的数量设置为5个，使用L1损失优化网络。</p>
<h4 id="不同频率的可视化">不同频率的可视化</h4>
<p><img src="/2021/02/16/SR_ORNet/6.png" style="zoom: 67%;"></p>
<p>对于FD模块，首先在图6(a)中对三个频率尺度的特征进行可视化，可以看到高频分支的特征包含更多的细节和纹理信息。然后根据图6(b)中的小波变换对低/中/高频特征进行分析。从左到右，频率范围从1增加到6中，低频特征的能量几乎完全集中在低频域，高频特征和中频特征的能量集中在相对更高的频率域。</p>
<h4 id="不同频率数量的比较">不同频率数量的比较</h4>
<p><img src="/2021/02/16/SR_ORNet/7.png" style="zoom:50%;"></p>
<p>提高频率分割的数量一定程度上能够提高效果。</p>
<h4 id="消融实验">消融实验</h4>
<p><img src="/2021/02/16/SR_ORNet/8.png" style="zoom: 50%;"></p>
<h4 id="与sota相比较">与SOTA相比较</h4>
<p><img src="/2021/02/16/SR_ORNet/9.png" style="zoom: 50%;"></p>
<p>其他方法除了CDC外均没有在DRealSR上训练，因此除了CDC外本质上无法证明该方法的有效性。</p>
<h3 id="结论">结论</h3>
<p>作者提出一种全频域自适应网络(OR-Net)来实现真实图像超分辨率。比较有贡献的地方在于分析了真实图像与图像频率的关系，以及频率在网络中的影响。然而现在RealSR大部分流行的方式是无监督学习（或unpaired的方式），而该文是完全的有监督学习。</p>
<p>有文章显示基于卷积的频率分离操作在PSNR和SSIM上稍高于基于小波变换的方法，但在视觉感知上（LPIPS指标）会稍差。对图像进行不同频率分离的方式会导致最终的重建效果有所不同。</p>
]]></content>
      <categories>
        <category>超分辨率</category>
        <category>SISR</category>
      </categories>
      <tags>
        <tag>SISR</tag>
        <tag>RealSR</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机网络传输层</title>
    <url>/2021/04/13/%E8%AE%A1%E7%BD%91_%E4%BC%A0%E8%BE%93%E5%B1%82/</url>
    <content><![CDATA[<h3 id="计算机网络传输层">计算机网络传输层</h3>
<p>TCP异常断开、TCP 粘包拆包</p>
<a id="more"></a>
<h4 id="tcp异常断开">TCP异常断开</h4>
<h5 id="服务器进程终止">服务器进程终止</h5>
<p>当服务器TCP接收到来自客户的数据时，既然先前打开那个套接口的进程已经终止，于是响应一个RST报文。</p>
<h5 id="服务器主机崩溃">服务器主机崩溃</h5>
<p>当服务器主机崩溃时，它不能发送任何东西，客户端TCP持续重传数据，试图从服务器上接受一个ACK。</p>
<ul>
<li>假设服务器已崩溃，从而对客户的数据分节根本没有响应，当客户端TCP最终放弃时，返回客户进程一个错误ETIMEDOUT。</li>
<li>如果某个中间路由器判定服务器主机已不可达，从而响应以一个“destination unreachable”，那么所返回的错误是EHOSTUNREACH或ENETUNREACH。</li>
</ul>
<p>为了快速检测出服务器已经不可达，有如下解决方案：</p>
<ul>
<li><p>给客户端设置一个超时时间，当超过这个时间后服务器还未响应数据时即认为服务器已经崩溃。</p></li>
<li><p>心跳检测机制，给双方设置一个守护进程，定期发送心跳检测数据包，另一方需要进行回应。</p></li>
<li><p>在Linux Socket编程中，开启SO_KEEPALIVE参数。</p>
<blockquote>
<p>SO_KEEPALIVE的过程？</p>
<p>1、如果通信两端超过2个小时没有交换数据，那么开启keep-alive的一端会自动发一个keep-alive包给对端。 2、如果对端正常的回应ACK包，那么一切都没问题，再等个2小时后发包(如果这两个小时仍然没有数据交换)。 3、如果对端回应RST包，表明对端程序崩溃或重启，这边socket产生ECONNRESET错误，并且关闭。 4、如果对端一直没回应，这边会每75秒再发包给对端，总共发8次共11分钟15秒。最后socket产生 ETIMEDOUT 错误，并且关闭。或者收到ICMP错误，表明主机不可到达，则会产生 EHOSTUNREACH 错误。</p>
<p>SO_KEEPALIVE和心跳机制的区别？</p>
<p>SO_KEEPALIVE是实现在传输层的TCP协议，心跳实现在应用层实现，本质没有任何区别，但应用层需要自己来定义心跳包格式。</p>
</blockquote></li>
</ul>
<h5 id="服务器主机崩溃后重启">服务器主机崩溃后重启</h5>
<p>当服务器主机崩溃后重启时，它的TCP丢失了崩溃前的所有连接信息，因此服务器TCP对于所收到的来自客户的数据响应以一个RST，客户端进程返回ECONNRESET错误。</p>
<h5 id="服务器主机关机">服务器主机关机</h5>
<p>Unix系统关机时，init进程通常先给所有进程发送SIGTERM信号（该信号可被捕获），再等待一段固定的时间（一般在5~20秒之间），然后给所有仍在运行的进程发送SIGKILL信号（该信号不能被捕获）。</p>
<p>这么做是留给所有运行中的进程一小段时间来清除和终止。</p>
<p>如果不捕获SIGTERM信号并终止，服务器将由SIGKILL信号终止。当服务器进程终止时，它的所有打开着的描述字都被关闭，随后发生的步骤与服务器主机崩溃一样。</p>
<h4 id="tcp-粘包拆包">TCP 粘包、拆包</h4>
<p>产生的原因：</p>
<ul>
<li><p>粘包</p>
<p>1、发送端要发送的数据小于发送缓冲区的大小，TCP 将多次写入缓冲区的数据一次发送出去</p>
<p>2、接收端应用层没有及时读取接收缓冲区中的数据</p></li>
<li><p>拆包</p>
<p>1、发送端要发送的数据大于发送缓冲区剩余空间大小</p>
<p>2、发送端待发送数据大于 MSS（最大报文长度）</p></li>
</ul>
<p>解决方案：</p>
<p>由于 TCP 本身是面向字节流的，无法理解上层的业务数据，所以在底层是无法保证数据包不被拆分和重组的，这个问题只能通过上层的应用层协议设计来解决。</p>
<ul>
<li><p>消息定长</p>
<p>发送端将每个数据包封装为固定长度（不够的可以通过补 0 填充），这样接收端每次接收缓冲区中读取固定长度的数据就自然而然的把每个数据包拆分开来。</p></li>
<li><p>设置消息边界</p>
<p>接收端按消息边界分离出消息内容。比如可以在包尾增加特殊符号（回车换行符等）进行分割，例如 FTP 协议。</p></li>
<li><p>在消息头中设置消息的长度</p>
<p>将消息分为消息头和消息体，消息头中包含表示消息总长度（或者消息体长度）的字段。</p></li>
<li><p>Netty协议</p></li>
</ul>
<h4 id="参考资料">参考资料</h4>
<p>https://zhuanlan.zhihu.com/p/108822858（粘包、拆包）</p>
<p>https://www.nowcoder.com/questionTerminal/72fa8f3cb67e4711ba8eee581f8be92b（TCP异常断开）</p>
<p>https://zhuanlan.zhihu.com/p/79957519 （异常断开）</p>
]]></content>
      <categories>
        <category>计算机网络</category>
        <category>传输层</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>传输层</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL日志</title>
    <url>/2021/04/02/MySQL_%E6%97%A5%E5%BF%97/</url>
    <content><![CDATA[<p>日志是MySQL数据库的重要组成部分，记录着数据库运行期间各种状态信息。</p>
<a id="more"></a>
<h4 id="binlog">binlog</h4>
<ul>
<li><p><strong>概念</strong></p>
<p>binlog属于逻辑日志，用于<strong>记录数据库执行的写入性操作</strong>(不包括查询)信息，以二进制的形式保存在磁盘中。binlog是MySQL的逻辑日志，并且由Server层进行记录，使用任何存储引擎的MySQL数据库都会记录binlog日志。</p>
<blockquote>
<p>逻辑日志：可以简单理解为记录的就是sql语句。但又不完全是sql语句这么简单，而是包括了执行的sql语句（增删改）反向的信息，也就意味着delete对应着delete本身和其反向的insert；update对应着update执行前后的版本的信息；insert对应着delete和insert本身的信息。</p>
<p>物理日志：因为MySQL数据最终是保存在数据页中的，物理日志记录的就是数据页变更，即数据本身的值。</p>
</blockquote></li>
<li><p><strong>使用场景</strong></p>
<p>binlog的主要使用场景有两个，分别是<strong>主从复制</strong>和<strong>数据恢复</strong>。</p>
<p>主从复制：在Master端开启binlog，然后将binlog发送到各个Slave端，Slave端重放binlog从而达到主从数据一致。</p>
<p><img src="/2021/04/02/MySQL_%E6%97%A5%E5%BF%97/3.png" style="zoom:50%;"></p>
<p>数据恢复：通过使用MySQL binlog工具来恢复数据。</p></li>
<li><p><strong>产生时机</strong></p>
<p>事务提交的时候，一次性将事务中的sql语句（一个事物可能对应多个sql语句）按照一定的格式记录到binlog中。</p></li>
<li><p><strong>释放时机</strong></p>
<p>binlog的默认是保持时间由参数expire_logs_days配置，也就是说对于非活动的日志文件，在生成时间超过expire_logs_days配置的天数之后，会被自动删除。</p></li>
<li><p>binlog记录方式</p>
<p>binlog是通过追加的方式进行写入的，可以通过max_binlog_size参数设置每个binlog文件的大小，当文件大小达到给定值之后，会生成新的文件来保存日志。</p></li>
</ul>
<h4 id="redo-log">redo log</h4>
<ul>
<li><p><strong>概念</strong></p>
<p>redo log属于物理日志，记录事务<strong>对数据页做了哪些修改</strong>。</p>
<blockquote>
<p>InnoDB的修改数据的基本流程</p>
<p>当我们想要修改DB上某一行数据的时候，InnoDB是把数据从磁盘读取到内存的缓冲池上进行修改。</p>
<p>这个时候数据在内存中被修改，与磁盘中相比就存在了差异，我们称这种有差异的数据为<strong>脏页</strong>。</p>
<p>InnoDB对脏页的处理不是每次生成脏页就将脏页刷新回磁盘，这样会产生海量的IO操作，严重影响InnoDB的处理性能。</p>
<p>既然脏页与磁盘中的数据存在差异，那么如果在这期间DB出现故障就会造成数据的丢失。为了解决这个问题，redo log就应运而生了。</p>
</blockquote>
<p>redo log包括两部分：一个是内存中的日志缓冲(redo log buffer)，另一个是磁盘上的日志文件(redo log file)。</p>
<p><img src="/2021/04/02/MySQL_%E6%97%A5%E5%BF%97/1.png" style="zoom:67%;"></p>
<p>mysql每执行一条DML语句，先将记录写入redo log buffer，后续某个时间点再一次性将多个操作记录写到redo log file。这种先写日志，再写磁盘的技术就是MySQL里经常说到的WAL(Write-Ahead Logging) 技术。</p></li>
<li><p><strong>使用场景</strong></p>
<p>redo log <strong>防止DB出现故障造成数据的丢失</strong>。当出现故障导致脏页还未来得及保存进磁盘时，通过redo log进行重做，从而达到事务的持久性这一特性。</p></li>
<li><p><strong>产生时机</strong></p>
<p>事务开始之后就产生redo log。</p></li>
<li><p><strong>释放时机</strong></p>
<p>当对应事务的脏页写入到磁盘之后，redo log的使命也就完成了，重做日志占用的空间就可以重用（被覆盖）。</p></li>
<li><p><strong>redo log记录方式</strong></p>
<p><img src="/2021/04/02/MySQL_%E6%97%A5%E5%BF%97/0.png" style="zoom:50%;"></p>
<p>redo log实际上记录数据页的变更，而这种变更记录是没必要全部保存，因此redo log实现上采用了大小固定，循环写入的方式，当写到结尾时，会回到开头循环写日志。</p>
<p>checkpoint所做的事就是把脏页给刷新回磁盘。所以，当DB重启恢复时，只需要恢复checkpoint之后的数据。这样就能大大缩短恢复时间。</p>
<p>check point到write pos之间是redo log待落盘的数据页更改记录。</p>
<p>write pos到check point之间的部分是redo log空着的部分，用于记录新的记录。</p>
<p>当write pos追上check point时，会先推动check point向前移动，空出位置再记录新的日志。</p></li>
<li><p><strong>redo log写入磁盘的时机</strong>（buffer到file的时机）</p>
<p>MySQL支持三种将redo log buffer写入redo log file的时机，可以通过innodb_flush_log_at_trx_commit参数配置，各参数值含义如下：</p>
<p>0（延迟写）</p>
<p>事务提交时不会将redo log buffer中日志写入到os buffer，而是每秒写入os buffer并调用fsync()写入到redo log file中。也就是说<strong>设置为0时是(大约)每秒刷新写入到磁盘中的</strong>，当系统崩溃，会丢失1秒钟的数据。</p>
<p>1（实时写，实时刷）</p>
<p>事务每次提交都会将redo log buffer中的日志写入os buffer并调用fsync()刷到redo log file中。这种方式即使系统崩溃也不会丢失任何数据，但是因为<strong>每次提交都写入磁盘</strong>，IO的性能较差。</p>
<p>2（实时写，延迟刷）</p>
<p>每次<strong>提交都仅写入</strong>到os buffer，然后是<strong>每秒调用</strong>fsync()将os buffer中的日志写入到redo log file。</p></li>
</ul>
<h4 id="undo-log">undo log</h4>
<ul>
<li><p>概念</p>
<p><img src="/2021/04/02/MySQL_%E6%97%A5%E5%BF%97/2.png" style="zoom: 67%;"></p>
<p>undo log属于逻辑日志，保存了事务发生之前的<strong>数据的一个版本</strong>，可以用于回滚。其实本质上，老旧的记录就形成了undo log。</p></li>
<li><p>使用场景</p>
<p>多版本并发控制协议MVCC中，通过undo log形成记录的多个版本，能执行快照读时能够判断读取的是哪个版本的记录。</p></li>
<li><p>产生时机</p>
<p>不同事务或者相同事务的对同一记录的修改，会导致该记录的undo log成为一条记录版本线性表，既链表。</p></li>
<li><p>释放时机</p>
<p>当事务提交之后，undo log并不能立马被删除，而是放入待清理的链表，由purge线程判断是否由其他事务在使用undo log上的旧版本，决定是否可以清理undo log的日志空间。</p></li>
</ul>
<h4 id="三者区别">三者区别</h4>
<table>
<colgroup>
<col style="width: 4%">
<col style="width: 31%">
<col style="width: 31%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>binlog</th>
<th>redo log</th>
<th>undo log</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>记录内容</td>
<td>用于<strong>记录数据库执行的写入性操作</strong>(不包括查询)信息，以二进制的形式保存在磁盘中。</td>
<td>记录事务<strong>对数据页做了哪些修改</strong>。</td>
<td>保存了事务发生之前的<strong>数据的一个版本</strong>，可以用于回滚。其实本质上，老旧的记录就形成了undo log。</td>
</tr>
<tr class="even">
<td>日志类型</td>
<td>逻辑日志</td>
<td>物理日志</td>
<td>逻辑日志</td>
</tr>
<tr class="odd">
<td>使用场景</td>
<td><strong>主从复制</strong>和<strong>数据恢复</strong>。主从复制：在Master端开启binlog，然后将binlog发送到各个Slave端，Slave端重放binlog从而达到主从数据一致。数据恢复：通过使用MySQL binlog工具来恢复数据。</td>
<td>redo log <strong>防止DB出现故障造成数据的丢失</strong>。当出现故障导致脏页还未来得及保存进磁盘时，通过redo log进行重做，从而达到事务的持久性这一特性。</td>
<td>多版本并发控制协议<strong>MVCC中的快照读</strong>，通过undo log形成记录的多个版本，能执行快照读时能够判断读取的是哪个版本的记录。</td>
</tr>
</tbody>
</table>
<p><strong>redo log 和 bin log 区别</strong></p>
<p>作用不同：redo log是保证事务的持久性的，binlog主要用于主从复制。</p>
<p>内容不同：redo log是物理日志，是数据页面的修改之后的物理记录，binlog是逻辑日志，可以简单认为记录的就是sql语句</p>
<p>产生时机不同：事务开始之后就产生redo log，事务提交的时候产生bin log。</p>
<p>释放时机不同：当对应事务的脏页写入到磁盘之后，redo log可以被移除，超过一定时间后，binlog被删除。</p>
<p>恢复效率：基于物理日志的redo log恢复数据的效率要高于语句逻辑日志的binlog。</p>
<p>先写redo log，再写binlog，两个日志都提交成功（刷入磁盘），事务才算真正的完成。</p>
<h4 id="参考资料">参考资料</h4>
<p>https://cloud.tencent.com/developer/article/1679325</p>
<p>https://www.cnblogs.com/wy123/p/8365234.html</p>
<p>https://zhuanlan.zhihu.com/p/35355751</p>
<p>https://www.cnblogs.com/xuwc/p/13873611.html</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>海量数据问题</title>
    <url>/2021/04/14/%E5%A4%A7%E6%95%B0%E6%8D%AE_%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>海量数据问题，就是数据量太大，所以导致要么是无法在较短时间内迅速解决，要么是数据太大导致无法一次性装入内存。</p>
<a id="more"></a>
<h3 id="基础">基础</h3>
<h4 id="换算单位">换算单位</h4>
<ul>
<li>1 byte = 8 bit</li>
<li>1 KB = <span class="math inline">\(2^{10}\)</span> byte = 1024 byte ≈ <span class="math inline">\(10^3\)</span> byte</li>
<li>1 MB = <span class="math inline">\(2^{20}\)</span> byte ≈ <span class="math inline">\(10^6\)</span> byte</li>
<li>1 GB = <span class="math inline">\(2^{30}\)</span> byte ≈ <span class="math inline">\(10^9\)</span> byte</li>
<li>1 千万 = <span class="math inline">\(10^7\)</span></li>
<li>1 亿 = <span class="math inline">\(10^8\)</span></li>
</ul>
<p>1 个整数占 4 byte，1 亿个整数占 4*<span class="math inline">\(10^8\)</span> byte ≈ 400 MB。</p>
<h4 id="位图bitmap">位图(bitmap)</h4>
<p>用一个bit位来标记某个元素对应的值。由于采用了Bit为单位来存储数据，因此在存储空间方面，可以大大节省。</p>
<p>想知道某个元素出现过没有。如果为每个所有可能的值分配1个bit。</p>
<p>但对于海量的、取值分布很均匀的集合进行<strong>去重</strong>，Bitmap极大地压缩了所需要的内存空间。于此同时，还额外地完成了对原始数组的<strong>排序工作</strong>。</p>
<p>统计数字出现的次数。</p>
<h4 id="bloom-filter布隆过滤器">Bloom Filter(布隆过滤器)</h4>
<h4 id="trie树">trie树</h4>
<p>前缀统计，词频统计。</p>
<h4 id="外部排序">外部排序</h4>
<p>大数据的排序，去重。</p>
<p>1、首先按内存大小，将外存上的文件分成若干子文件。</p>
<p>2、依次读入内存并利用有效的内部排序对他们进行排序，并将排序后得到的有序字文件重新写入外存，通常称这些子文件为归并段。</p>
<p>3、对这些归并段进行逐趟归并，使归并段逐渐由小到大，直至得到整个有序文件。</p>
<p>外排序的优化方法：置换选择 败者树原理，最优归并树。</p>
<h3 id="常用解决方案">常用解决方案</h3>
<p>1、分而治之/hash映射 + hash统计 + 堆/快速/归并排序； 2、Bitmap去重；</p>
<p>当分而治之（hash映射）出现数据倾斜：改变hash函数再次进行映射。</p>
<p>找数字的去重用位图，字符串去重用HashSet。</p>
<h4 id="海量数据去重">海量数据去重</h4>
<p>数字去重使用位图bitmap，字符串去重HashSet。</p>
<h4 id="n个数求前k大">N个数求前K大</h4>
<p>1、排序算法。 2、计数排序，开辟一个大数组，记录每个整数是否出现，从大到小取。（bitmap） 3、维护一个大小为k的小顶堆。</p>
<h4 id="求文件a中没有但b中有的单词">求文件A中没有但B中有的单词</h4>
<p>遍历文件A，将文件hash到n个小文件中，对B文件同样操作。然后对于每一对文件，先将一个文件存入HashSet，对另一个文件遍历判断。</p>
<h4 id="海量数据排序问题">海量数据排序问题</h4>
<p>hash到小文件中（其实直接切分成若干小文件也行），然后先在小文件排序。</p>
<p>然后合并的时候，用堆，每个小文件取一个，然后最小的拿走再加入对应文件的数字，直到结束。</p>
<p>又或者直接对多个小文件进行归并排序。</p>
<h4 id="海量数据出现次数最多数据">海量数据出现次数最多数据</h4>
<p>分而治之/hash映射（相同数据被分到一起） + HashMap统计 + 堆/快速/归并排序；</p>
<h3 id="面试题">面试题</h3>
<h4 id="字符串统计次数海量日志数据提取出某日访问百度次数最多的那个ip">字符串统计次数：海量日志数据，提取出某日访问百度次数最多的那个IP</h4>
<p><strong>算法思想：分而治之+Hash统计+排序</strong></p>
<p>IP地址最多有<span class="math inline">\(2^{32}\)</span>种取值情况，即约40亿种取值。每个ip地址是32位，因此是一个ip地址大小为4 byte。所以所有ip地址的大小为 <span class="math inline">\(40*10^8*4 byte= 16GB\)</span>， 不能完全加载到内存中处理。</p>
<p>1、按照IP地址的Hash(IP)%1024值，把海量IP日志分别存储到1024个小文件中。这样，每个小文件最多包含<span class="math inline">\(2^{22}=4*2^{20}\)</span>个IP地址，即约400万个ip地址，每个小文件大小为<span class="math inline">\(4*2^{20}*4bytes=16MB\)</span>。</p>
<p>2、对于每一个小文件，可以构建一个IP为key，出现次数为value的Hash map，同时记录当前出现次数最多的那个IP地址。</p>
<p>3、可以得到1024个小文件中的出现次数最多的IP，再依据常规的排序算法得到总体上出现次数最多的IP。</p>
<h4 id="top-k统计最热门的10个查询串">TOP K：统计最热门的10个查询串</h4>
<p>搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。），请你统计最热门的10个查询串，要求使用的内存不能超过1G。</p>
<p><strong>算法思想：hashmap+堆</strong></p>
<p>去重后300万个查询串的大小最大为<span class="math inline">\(3*10^6*255byte = 765MB\)</span>，可以放入内存当中，构建HashMap统计次数时，value为整型，因此value所占的空间大小为<span class="math inline">\(3*10^6*4byte=12MB\)</span>，因此总内存可以装载这个HashMap。</p>
<p>1、先对这批海量数据预处理，在O(N)的时间内用Hash表完成统计； 2、借助堆这个数据结构，找出Top K，时间复杂度为O(N*logK)。</p>
<h4 id="数字找不重复数字统计次数海量数据找出不重复的整数">数字找不重复/数字统计次数：海量数据找出不重复的整数</h4>
<p>在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。</p>
<p><strong>算法思想：bitmap位图去重/统计次数</strong></p>
<p>1、2-Bitmap，每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义，共需内存<span class="math inline">\(2^{32} * 2 bit = 2^{32} * 0.25 byte =1 GB\)</span>内存。</p>
<p>2、然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。</p>
<p>3、扫描完成后，查看bitmap，把对应位是01的整数输出即可。</p>
<p>如果需要统计数字出现的次数，则只需要增加bitmap中每一位的bit数即可。</p>
<h4 id="找重复找出ab文件共同的url">找重复：找出a、b文件共同的url</h4>
<p>给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url。</p>
<p><strong>算法思想：分而治之 + HashSet去重</strong></p>
<p>每个文件的大小约为<span class="math inline">\(50*10^8*64byte=64*5GB\)</span>，无法放入内存种。</p>
<p>1、遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件中。这样每个小文件的大约为300M。遍历文件b，采取和a相同的方式将url分别存储到1000小文件。</p>
<p>2、这样处理后，所有可能相同的url都在对应的小文件中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。</p>
<p>3、求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。</p>
<h4 id="外部排序10个文件按照query频度排序">外部排序：10个文件，按照query频度排序</h4>
<p>有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求按照query的频度排序。</p>
<p><strong>算法思想：分而治之 + hash统计 + 内外排序</strong></p>
<p>1、顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件中。这样新生成的文件每个的大小大约也1G，大于1G的更换哈希函数继续进行切分。</p>
<p>2、找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。利用排序算法按照出现次数进行排序。将排序好的query和对应的query_cout输出到文件中。这样得到了10个排好序的文件。</p>
<p>3、对这10个文件进行归并排序。</p>
<h3 id="参考资料">参考资料</h3>
<p>https://wangpengcheng.github.io/2019/12/17/hailiangshuju_problems/</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>海量数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>海量数据</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL窗口函数</title>
    <url>/2021/04/13/MySQL_%E7%AA%97%E5%8F%A3/</url>
    <content><![CDATA[<p>窗口指的是记录的集合。窗口函数也就是在满足某种条件的记录集合上执行的特殊函数。</p>
<a id="more"></a>
<h4 id="基本用法">基本用法</h4>
<blockquote>
<p>函数名 OVER (子句)</p>
</blockquote>
<p>函数名部分即表示窗口函数。</p>
<p>over关键字用来指定函数执行的窗口范围，若后面括号中什么都不写，则窗口函数基于所有行进行计算；如果不为空，则支持以下4中语法来设置窗口：</p>
<ul>
<li><p><code>PARTITION BY</code> 子句：窗口按照哪些字段进行分组，窗口函数在不同的分组上分别执行；</p></li>
<li><p><code>ORDER BY</code>子句：按照哪些字段进行排序，窗口函数将按照排序后的记录顺序进行编号；</p></li>
<li><p><code>FRAME</code>子句：FRAME是当前分区的一个子集，子句用来定义子集的规则，通常用来作为滑动窗口使用</p></li>
<li><p>window_name：给窗口指定一个别名。如果SQL中涉及的窗口较多，采用别名可以看起来更清晰易读；</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="built_in">rank</span> ( ) <span class="keyword">over</span> w1 </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">    employee <span class="keyword">window</span> w1 <span class="keyword">AS</span> ( <span class="keyword">PARTITION</span> <span class="keyword">BY</span> Company <span class="keyword">ORDER</span> <span class="keyword">BY</span> Salary <span class="keyword">DESC</span> )</span><br><span class="line">    </span><br><span class="line"># <span class="keyword">from</span> <span class="keyword">table</span>后面加了一个关键字<span class="keyword">WINDOW</span>，后面跟了一个子句，这样在<span class="keyword">select</span>中就可以使用这个w1作为窗口了。</span><br></pre></td></tr></table></figure>
<h4 id="窗口函数">窗口函数</h4></li>
</ul>
<p>窗口函数，在某些记录的集合上执行的函数。</p>
<h5 id="序号函数">序号函数</h5>
<p>row_number()/rank()/dense_rank()</p>
<p>作用：显示分区中的当前行号（排名是需要通过<code>ORDER BY</code>子句实现的）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span>,</span><br><span class="line">   <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> 成绩 <span class="keyword">desc</span>) <span class="keyword">as</span> ranking,</span><br><span class="line">   <span class="built_in">dense_rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> 成绩 <span class="keyword">desc</span>) <span class="keyword">as</span> dese_rank,</span><br><span class="line">   <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> 成绩 <span class="keyword">desc</span>) <span class="keyword">as</span> row_num</span><br><span class="line"><span class="keyword">from</span> 班级;</span><br></pre></td></tr></table></figure>
<p><img src="/2021/04/13/MySQL_%E7%AA%97%E5%8F%A3/0.png" style="zoom:67%;"></p>
<ul>
<li><p>rank函数：如果有并列名次的行，会占用下一名次的位置。比如正常排名是1，2，3，4，但是现在前3名是并列的名次，结果是：1，1，1，4。</p></li>
<li><p>dense_rank函数：如果有并列名次的行，不占用下一名次的位置。比如正常排名是1，2，3，4，但是现在前3名是并列的名次，结果是：1，1，1，2。</p></li>
<li><p>row_number函数：不考虑并列名次的情况。比如前3名是并列的名次，排名是正常的1，2，3，4。</p></li>
</ul>
<h5 id="分布函数">分布函数</h5>
<p>percent_rank()/cume_dist()</p>
<p>percent_rank()</p>
<p>用途：和之前的RANK()函数相关，每行按照公式进行计算：(rank - 1) / (rows - 1)</p>
<p>其中，rank为RANK()函数产生的序号，rows为当前窗口的记录总行数该函数可以用来计算分位数。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="built_in">percent_rank</span> ( ) <span class="keyword">over</span> ( <span class="keyword">PARTITION</span> <span class="keyword">BY</span> Company <span class="keyword">ORDER</span> <span class="keyword">BY</span> Salary <span class="keyword">DESC</span> ),</span><br><span class="line">    Company,</span><br><span class="line">    Salary </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">    employee </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">    Company <span class="operator">=</span> <span class="string">&#x27;Microsoft&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src="/2021/04/13/MySQL_%E7%AA%97%E5%8F%A3/1.png" style="zoom:67%;"></p>
<p>cume_dist()</p>
<p>用途：分组内小于等于当前rank值的行数/分组内总行数。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="built_in">cume_dist</span> ( ) <span class="keyword">over</span> ( <span class="keyword">PARTITION</span> <span class="keyword">BY</span> Company <span class="keyword">ORDER</span> <span class="keyword">BY</span> Salary <span class="keyword">DESC</span> ),</span><br><span class="line">    Company,</span><br><span class="line">    Salary </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">    employee </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">    Company <span class="operator">=</span> <span class="string">&#x27;Microsoft&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src="/2021/04/13/MySQL_%E7%AA%97%E5%8F%A3/2.png" style="zoom:67%;"></p>
<h5 id="聚合函数">聚合函数</h5>
<p>sum/avg/max/min/count</p>
<p>用途：在窗口中每条记录动态应用聚合函数(sum/avg/max/min/count)，可以动态计算在指定的窗口内的各种聚合函数值。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="built_in">avg</span>( Salary ) <span class="keyword">over</span> ( <span class="keyword">PARTITION</span> <span class="keyword">BY</span> Company <span class="keyword">ORDER</span> <span class="keyword">BY</span> Salary <span class="keyword">DESC</span> ),</span><br><span class="line">    Company,</span><br><span class="line">    Salary </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">    employee </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">    Company <span class="operator">=</span> <span class="string">&#x27;Microsoft&#x27;</span></span><br></pre></td></tr></table></figure>
<p>按照分组的每一行，求累计的平均值。根据over子句分组之后，挨个行进行select，并执行over前面的函数，因为over子句已经把记录变成了符合条件的一些行集，所以select的方式就改变了。</p>
<p><img src="/2021/04/13/MySQL_%E7%AA%97%E5%8F%A3/3.png" style="zoom:67%;"></p>
<h4 id="参考资料">参考资料</h4>
<p>https://leetcode-cn.com/problems/department-top-three-salaries/solution/tu-jie-sqlmian-shi-ti-jing-dian-topnwen-ti-by-houz/</p>
<p>https://www.jianshu.com/p/e5c5bfb1e28b</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
</search>
